"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[960],{5023:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"part5-advanced/human-robot-interaction","title":"Chapter 19: Human-Robot Interaction","description":"Learning Objectives","source":"@site/docs/part5-advanced/19-human-robot-interaction.md","sourceDirName":"part5-advanced","slug":"/part5-advanced/human-robot-interaction","permalink":"/robot_book/docs/part5-advanced/human-robot-interaction","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":19,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 18: Whole-Body Control","permalink":"/robot_book/docs/part5-advanced/whole-body-control"},"next":{"title":"Chapter 20: Safety and Compliance","permalink":"/robot_book/docs/part5-advanced/safety-compliance"}}');var t=i(4848),s=i(8453);const o={},a="Chapter 19: Human-Robot Interaction",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. HRI Modalities",id:"1-hri-modalities",level:2},{value:"2. Speech Interface",id:"2-speech-interface",level:2},{value:"Install Dependencies",id:"install-dependencies",level:3},{value:"Speech-to-Text",id:"speech-to-text",level:3},{value:"Text-to-Speech",id:"text-to-speech",level:3},{value:"3. Gesture Recognition",id:"3-gesture-recognition",level:2},{value:"MediaPipe Hand Tracking",id:"mediapipe-hand-tracking",level:3},{value:"4. Teleoperation",id:"4-teleoperation",level:2},{value:"VR Controller Integration",id:"vr-controller-integration",level:3},{value:"5. Multimodal Integration",id:"5-multimodal-integration",level:2},{value:"Command Fusion",id:"command-fusion",level:3},{value:"6. Safety in HRI",id:"6-safety-in-hri",level:2},{value:"Proxemics (Personal Space)",id:"proxemics-personal-space",level:3},{value:"Emergency Stop",id:"emergency-stop",level:3},{value:"7. Hands-On Lab: Voice-Controlled Grasping (3 hours)",id:"7-hands-on-lab-voice-controlled-grasping-3-hours",level:2},{value:"8. End-of-Chapter Project",id:"8-end-of-chapter-project",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-19-human-robot-interaction",children:"Chapter 19: Human-Robot Interaction"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," teleoperation interfaces for humanoid control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate"})," speech recognition and synthesis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy"})," gesture recognition for natural interaction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build"})," safety-aware HRI behaviors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluate"})," user experience and interaction quality"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"1-hri-modalities",children:"1. HRI Modalities"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Input Channels"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Speech: "Pick up the red cup"'}),"\n",(0,t.jsx)(n.li,{children:"Gestures: Pointing, hand signals"}),"\n",(0,t.jsx)(n.li,{children:"Teleoperation: VR controllers, joysticks"}),"\n",(0,t.jsx)(n.li,{children:"GUI: Touchscreen interfaces"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output Channels"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech: Text-to-speech responses"}),"\n",(0,t.jsx)(n.li,{children:"Display: Status information"}),"\n",(0,t.jsx)(n.li,{children:"Motion: Acknowledge with head nod"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-speech-interface",children:"2. Speech Interface"}),"\n",(0,t.jsx)(n.h3,{id:"install-dependencies",children:"Install Dependencies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install speechrecognition pyttsx3\nsudo apt install portaudio19-dev\n"})}),"\n",(0,t.jsx)(n.h3,{id:"speech-to-text",children:"Speech-to-Text"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\n\ndef listen_for_command():\n    """Capture speech and convert to text."""\n    recognizer = sr.Recognizer()\n\n    with sr.Microphone() as source:\n        print("Listening...")\n        audio = recognizer.listen(source, timeout=5)\n\n    try:\n        command = recognizer.recognize_google(audio)\n        return command\n    except sr.UnknownValueError:\n        return None\n\n# Usage\ncommand = listen_for_command()\nprint(f"Command: {command}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"text-to-speech",children:"Text-to-Speech"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import pyttsx3\n\ndef speak(text):\n    """Convert text to speech."""\n    engine = pyttsx3.init()\n    engine.setProperty(\'rate\', 150)  # Speed\n    engine.say(text)\n    engine.runAndWait()\n\nspeak("Task completed successfully")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-gesture-recognition",children:"3. Gesture Recognition"}),"\n",(0,t.jsx)(n.h3,{id:"mediapipe-hand-tracking",children:"MediaPipe Hand Tracking"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport mediapipe as mp\n\nclass GestureRecognizer:\n    def __init__(self):\n        self.mp_hands = mp.solutions.hands\n        self.hands = self.mp_hands.Hands(\n            max_num_hands=1,\n            min_detection_confidence=0.7\n        )\n\n    def detect_gesture(self, image):\n        """Detect hand gesture from image."""\n        results = self.hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        if not results.multi_hand_landmarks:\n            return None\n\n        # Get hand landmarks\n        landmarks = results.multi_hand_landmarks[0]\n\n        # Classify gesture (simplified)\n        if self.is_pointing(landmarks):\n            return "POINT"\n        elif self.is_thumbs_up(landmarks):\n            return "APPROVE"\n        elif self.is_stop_sign(landmarks):\n            return "STOP"\n\n        return None\n\n    def is_pointing(self, landmarks):\n        # Index finger extended, others closed\n        index_tip = landmarks.landmark[8]\n        index_mcp = landmarks.landmark[5]\n        return index_tip.y < index_mcp.y  # Simplified\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-teleoperation",children:"4. Teleoperation"}),"\n",(0,t.jsx)(n.h3,{id:"vr-controller-integration",children:"VR Controller Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import Twist\n\nclass TeleoperationNode(Node):\n    def __init__(self):\n        super().__init__('teleoperation')\n\n        # Subscribe to VR controller\n        self.create_subscription(Twist, '/vr_controller/cmd_vel', self.vr_callback, 10)\n\n        # Publish to robot\n        self.joint_pub = self.create_publisher(JointState, '/joint_commands', 10)\n\n    def vr_callback(self, msg):\n        \"\"\"Map VR controller input to robot joints.\"\"\"\n        # Map linear velocity to walking\n        # Map angular velocity to turning\n\n        joint_msg = JointState()\n        # ... (mapping logic)\n        self.joint_pub.publish(joint_msg)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"5-multimodal-integration",children:"5. Multimodal Integration"}),"\n",(0,t.jsx)(n.h3,{id:"command-fusion",children:"Command Fusion"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class MultimodalInterface:\n    def __init__(self):\n        self.speech = SpeechRecognizer()\n        self.gesture = GestureRecognizer()\n        self.vla = OpenVLA()\n\n    def process_command(self, image, audio):\n        """Combine speech and gesture for robust commands."""\n\n        # Speech: "Pick up the"\n        speech_cmd = self.speech.recognize(audio)\n\n        # Gesture: Pointing at object\n        gesture = self.gesture.detect(image)\n\n        # Combine\n        if "pick up" in speech_cmd and gesture == "POINT":\n            # Use VLA with pointing location\n            target = self.get_pointed_object(image, gesture)\n            action = self.vla.predict_action(\n                image,\n                f"Pick up the object at {target}"\n            )\n            return action\n\n        return None\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"6-safety-in-hri",children:"6. Safety in HRI"}),"\n",(0,t.jsx)(n.h3,{id:"proxemics-personal-space",children:"Proxemics (Personal Space)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def check_human_proximity(human_pos, robot_pos):\n    """Slow down if human is too close."""\n    distance = np.linalg.norm(human_pos - robot_pos)\n\n    if distance < 0.5:  # < 0.5m: Stop\n        return 0.0\n    elif distance < 1.0:  # 0.5-1m: Slow\n        return 0.3\n    else:  # > 1m: Normal\n        return 1.0\n\nspeed_factor = check_human_proximity(human, robot)\nrobot.set_velocity_scale(speed_factor)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"emergency-stop",children:"Emergency Stop"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SafetyMonitor:\n    def __init__(self):\n        self.emergency_phrases = ["stop", "freeze", "emergency"]\n\n    def check_speech_command(self, command):\n        """Emergency stop on safety keywords."""\n        if any(phrase in command.lower() for phrase in self.emergency_phrases):\n            robot.emergency_stop()\n            speak("Stopping immediately")\n            return True\n        return False\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"7-hands-on-lab-voice-controlled-grasping-3-hours",children:"7. Hands-On Lab: Voice-Controlled Grasping (3 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Control robot grasping with voice commands."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Setup speech recognition"}),"\n",(0,t.jsx)(n.li,{children:"Integrate with VLA (Chapter 16)"}),"\n",(0,t.jsx)(n.li,{children:"Add confirmation responses (TTS)"}),"\n",(0,t.jsx)(n.li,{children:'Test commands: "Pick up the cup", "Put down the bottle"'}),"\n",(0,t.jsx)(n.li,{children:"Measure task success rate"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Validation"}),": >70% command recognition, >60% task success"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"8-end-of-chapter-project",children:"8. End-of-Chapter Project"}),"\n",(0,t.jsx)(n.p,{children:"Build multimodal HRI system for household assistance."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech interface (3+ commands)"}),"\n",(0,t.jsx)(n.li,{children:"Gesture recognition (point, approve, stop)"}),"\n",(0,t.jsx)(n.li,{children:"VLA execution"}),"\n",(0,t.jsx)(n.li,{children:"Safety monitoring"}),"\n",(0,t.jsx)(n.li,{children:"User study with 3 participants"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverables"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"HRI system code"}),"\n",(0,t.jsx)(n.li,{children:"User study results (task completion time, errors)"}),"\n",(0,t.jsx)(n.li,{children:"Demo video (3 multimodal interactions)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Human-robot interaction combines speech, gestures, and visual interfaces for natural robot control. Safety-aware behaviors ensure comfortable human-robot collaboration."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next"}),": Chapter 20 covers safety standards and compliance for real-world deployment."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(6540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);