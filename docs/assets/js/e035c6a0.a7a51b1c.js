"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[488],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}},9620:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"part3-perception/object-detection-yolo","title":"Chapter 10: Object Detection with YOLO","description":"Learning Objectives","source":"@site/docs/part3-perception/10-object-detection-yolo.md","sourceDirName":"part3-perception","slug":"/part3-perception/object-detection-yolo","permalink":"/robot_book/docs/part3-perception/object-detection-yolo","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":10,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 9: Intel RealSense Integration","permalink":"/robot_book/docs/part3-perception/realsense-integration"},"next":{"title":"Chapter 11: Jetson Orin Deployment","permalink":"/robot_book/docs/part3-perception/jetson-orin-deployment"}}');var o=t(4848),s=t(8453);const r={},l="Chapter 10: Object Detection with YOLO",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. YOLO Overview",id:"1-yolo-overview",level:2},{value:"2. Installation",id:"2-installation",level:2},{value:"3. Basic Object Detection",id:"3-basic-object-detection",level:2},{value:"Python API",id:"python-api",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"4. Training Custom Models",id:"4-training-custom-models",level:2},{value:"Prepare Dataset (COCO Format)",id:"prepare-dataset-coco-format",level:3},{value:"Train YOLOv8",id:"train-yolov8",level:3},{value:"5. TensorRT Optimization",id:"5-tensorrt-optimization",level:2},{value:"6. 3D Object Localization (RGBD Fusion)",id:"6-3d-object-localization-rgbd-fusion",level:2},{value:"7. Hands-On Lab: Tabletop Object Detection (3 hours)",id:"7-hands-on-lab-tabletop-object-detection-3-hours",level:2},{value:"8. End-of-Chapter Project: Pick-and-Place Vision",id:"8-end-of-chapter-project-pick-and-place-vision",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-10-object-detection-with-yolo",children:"Chapter 10: Object Detection with YOLO"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Install"})," YOLOv8 and integrate with ROS 2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Detect"})," objects in real-time (30 FPS on GPU)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Train"})," custom models for humanoid-specific objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Optimize"})," for edge deployment (TensorRT)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Combine"})," with RealSense depth for 3D object localization"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"1-yolo-overview",children:"1. YOLO Overview"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"YOLOv8"})," (Ultralytics, 2023):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speed"}),": 30-60 FPS on RTX 4070"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy"}),": 50+ mAP on COCO dataset"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Classes"}),": 80 pre-trained (person, cup, bottle, etc.)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Variants"}),": YOLOv8n (nano), YOLOv8s (small), YOLOv8m (medium)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Why YOLO for Humanoids"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Real-time performance"}),"\n",(0,o.jsx)(n.li,{children:"Single-stage detector (fast)"}),"\n",(0,o.jsx)(n.li,{children:"Easy fine-tuning for custom objects"}),"\n",(0,o.jsx)(n.li,{children:"TensorRT support for Jetson"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"2-installation",children:"2. Installation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Install Ultralytics\npip install ultralytics\n\n# Verify\nyolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'\n# Should download model and detect objects in bus.jpg\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"3-basic-object-detection",children:"3. Basic Object Detection"}),"\n",(0,o.jsx)(n.h3,{id:"python-api",children:"Python API"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Load pre-trained model\nmodel = YOLO('yolov8n.pt')  # Nano (fastest)\n\n# Run inference\nresults = model.predict(source='image.jpg', conf=0.5)\n\n# Process results\nfor result in results:\n    boxes = result.boxes\n    for box in boxes:\n        x1, y1, x2, y2 = box.xyxy[0]  # Bounding box\n        conf = box.conf[0]              # Confidence\n        cls = box.cls[0]                # Class ID\n        print(f\"Detected {model.names[int(cls)]} at ({x1}, {y1}) conf={conf:.2f}\")\n"})}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom cv_bridge import CvBridge\nfrom ultralytics import YOLO\n\nclass YOLODetector(Node):\n    def __init__(self):\n        super().__init__('yolo_detector')\n        self.model = YOLO('yolov8n.pt')\n        self.bridge = CvBridge()\n\n        self.subscription = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10)\n        self.publisher = self.create_publisher(Detection2DArray, '/detections', 10)\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Run YOLO\n        results = self.model.predict(cv_image, verbose=False)\n\n        # Publish detections\n        detections_msg = Detection2DArray()\n        for result in results:\n            for box in result.boxes:\n                det = Detection2D()\n                det.bbox.center.x = float((box.xyxy[0][0] + box.xyxy[0][2]) / 2)\n                det.bbox.center.y = float((box.xyxy[0][1] + box.xyxy[0][3]) / 2)\n                det.bbox.size_x = float(box.xyxy[0][2] - box.xyxy[0][0])\n                det.bbox.size_y = float(box.xyxy[0][3] - box.xyxy[0][1])\n                detections_msg.detections.append(det)\n\n        self.publisher.publish(detections_msg)\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"4-training-custom-models",children:"4. Training Custom Models"}),"\n",(0,o.jsx)(n.h3,{id:"prepare-dataset-coco-format",children:"Prepare Dataset (COCO Format)"}),"\n",(0,o.jsx)(n.p,{children:"Use Chapter 7 synthetic data generator:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Generate 1000 images\n./python.sh generate_grasping_dataset.py --num-images 1000\n\n# Convert to COCO format\npython3 convert_to_coco.py --input-dir datasets/grasping_dataset \\\n  --output-file annotations.json\n"})}),"\n",(0,o.jsx)(n.h3,{id:"train-yolov8",children:"Train YOLOv8"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Load base model\nmodel = YOLO('yolov8n.pt')\n\n# Train on custom data\nmodel.train(\n    data='config.yaml',  # Dataset config\n    epochs=50,\n    imgsz=640,\n    batch=16,\n    device=0  # GPU 0\n)\n\n# Validate\nmetrics = model.val()\nprint(f\"mAP50: {metrics.box.map50}\")\n\n# Export to TensorRT (for Jetson)\nmodel.export(format='engine', device=0)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"config.yaml"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"path: /path/to/dataset\ntrain: images/train\nval: images/val\n\nnames:\n  0: cube\n  1: cylinder\n  2: sphere\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"5-tensorrt-optimization",children:"5. TensorRT Optimization"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"For Jetson Orin"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Export to TensorRT engine\nyolo export model=yolov8n.pt format=engine device=0\n\n# Inference with TensorRT\nmodel = YOLO('yolov8n.engine')\nresults = model.predict('image.jpg')\n# 2-3x faster than PyTorch on Jetson\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance"})," (Jetson Orin Nano):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"PyTorch FP32: 12 FPS"}),"\n",(0,o.jsx)(n.li,{children:"TensorRT FP16: 35 FPS"}),"\n",(0,o.jsx)(n.li,{children:"TensorRT INT8: 50 FPS (with calibration)"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"6-3d-object-localization-rgbd-fusion",children:"6. 3D Object Localization (RGBD Fusion)"}),"\n",(0,o.jsx)(n.p,{children:"Combine YOLO (2D) with RealSense depth (3D):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def get_3d_position(bbox, depth_image, camera_intrinsics):\n    """\n    Convert 2D bounding box + depth to 3D position.\n\n    Args:\n        bbox: (x1, y1, x2, y2) in pixels\n        depth_image: Depth map (H, W) in mm\n        camera_intrinsics: (fx, fy, cx, cy)\n\n    Returns:\n        (x, y, z) in meters (camera frame)\n    """\n    # Get bbox center\n    cx_bbox = int((bbox[0] + bbox[2]) / 2)\n    cy_bbox = int((bbox[1] + bbox[3]) / 2)\n\n    # Get depth at center\n    depth_mm = depth_image[cy_bbox, cx_bbox]\n    z = depth_mm / 1000.0  # Convert to meters\n\n    # Unproject to 3D\n    fx, fy, cx, cy = camera_intrinsics\n    x = (cx_bbox - cx) * z / fx\n    y = (cy_bbox - cy) * z / fy\n\n    return (x, y, z)\n\n# Example usage\nbbox = [100, 150, 200, 250]  # Detected cup\nposition_3d = get_3d_position(bbox, depth_image, (615, 615, 320, 240))\nprint(f"Cup position: {position_3d} meters from camera")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"7-hands-on-lab-tabletop-object-detection-3-hours",children:"7. Hands-On Lab: Tabletop Object Detection (3 hours)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Goal"}),": Detect and localize objects on table for grasping."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Launch RealSense + YOLO detector"}),"\n",(0,o.jsx)(n.li,{children:"Detect objects (cups, bottles)"}),"\n",(0,o.jsx)(n.li,{children:"Get 3D positions using depth"}),"\n",(0,o.jsxs)(n.li,{children:["Publish to ",(0,o.jsx)(n.code,{children:"/objects_3d"})," topic"]}),"\n",(0,o.jsx)(n.li,{children:"Visualize in RViz2"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Validation"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Detects \u22653 objects on table"}),"\n",(0,o.jsx)(n.li,{children:"3D positions accurate to \xb15cm"}),"\n",(0,o.jsx)(n.li,{children:"Runs at \u226520 FPS"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"8-end-of-chapter-project-pick-and-place-vision",children:"8. End-of-Chapter Project: Pick-and-Place Vision"}),"\n",(0,o.jsx)(n.p,{children:"Build vision system for humanoid pick-and-place task."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Detect graspable objects (cups, bottles, bowls)"}),"\n",(0,o.jsx)(n.li,{children:"Filter objects within reach (0.3-0.8m)"}),"\n",(0,o.jsx)(n.li,{children:"Rank by grasp confidence (size, orientation)"}),"\n",(0,o.jsxs)(n.li,{children:["Publish top candidate to ",(0,o.jsx)(n.code,{children:"/grasp_target"})]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Deliverables"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ROS 2 package with YOLO + depth fusion"}),"\n",(0,o.jsx)(n.li,{children:"Launch file"}),"\n",(0,o.jsx)(n.li,{children:"2-minute demo video"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"You learned to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Install YOLOv8 and integrate with ROS 2"}),"\n",(0,o.jsx)(n.li,{children:"Detect objects in real-time at 30+ FPS"}),"\n",(0,o.jsx)(n.li,{children:"Train custom models on synthetic data"}),"\n",(0,o.jsx)(n.li,{children:"Optimize with TensorRT for Jetson"}),"\n",(0,o.jsx)(n.li,{children:"Fuse 2D detections with 3D depth data"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Next"}),": Chapter 11 covers deploying to Jetson Orin with model quantization."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);