"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[213],{1659:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"part4-vla/openvla-finetuning","title":"Chapter 14: OpenVLA Fine-Tuning","description":"Learning Objectives","source":"@site/docs/part4-vla/14-openvla-finetuning.md","sourceDirName":"part4-vla","slug":"/part4-vla/openvla-finetuning","permalink":"/robot_book/docs/part4-vla/openvla-finetuning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":14,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 13: VLA Architecture Fundamentals","permalink":"/robot_book/docs/part4-vla/vla-architecture"},"next":{"title":"Chapter 15: Multimodal Reasoning","permalink":"/robot_book/docs/part4-vla/multimodal-reasoning"}}');var s=i(4848),t=i(8453);const a={},l="Chapter 14: OpenVLA Fine-Tuning",o={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. Why Fine-Tune?",id:"1-why-fine-tune",level:2},{value:"2. Dataset Preparation",id:"2-dataset-preparation",level:2},{value:"Data Collection",id:"data-collection",level:3},{value:"Data Format (RLDS)",id:"data-format-rlds",level:3},{value:"3. Fine-Tuning with LoRA",id:"3-fine-tuning-with-lora",level:2},{value:"Training Script",id:"training-script",level:3},{value:"4. Training Configuration",id:"4-training-configuration",level:2},{value:"5. Evaluation Metrics",id:"5-evaluation-metrics",level:2},{value:"Success Rate",id:"success-rate",level:3},{value:"Action Error",id:"action-error",level:3},{value:"6. Deployment",id:"6-deployment",level:2},{value:"7. Hands-On Lab: Fine-Tune on Grasping (4 hours)",id:"7-hands-on-lab-fine-tune-on-grasping-4-hours",level:2},{value:"8. End-of-Chapter Project",id:"8-end-of-chapter-project",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-14-openvla-fine-tuning",children:"Chapter 14: OpenVLA Fine-Tuning"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Prepare"})," custom robot datasets for VLA training"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fine-tune"})," OpenVLA on humanoid-specific tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Optimize"})," training with LoRA for efficiency"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Evaluate"})," model performance on held-out tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Deploy"})," fine-tuned models to real robots"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"1-why-fine-tune",children:"1. Why Fine-Tune?"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Pre-trained VLA Limitations"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Trained on arm manipulators (not humanoids)"}),"\n",(0,s.jsx)(e.li,{children:"Limited to tabletop tasks"}),"\n",(0,s.jsx)(e.li,{children:"May not understand your robot's morphology"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Fine-Tuning Benefits"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Adapt to humanoid kinematics (12-DOF)"}),"\n",(0,s.jsx)(e.li,{children:"Learn domain-specific skills (bipedal grasping)"}),"\n",(0,s.jsx)(e.li,{children:"Improve success rate from 40% \u2192 80%"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"2-dataset-preparation",children:"2. Dataset Preparation"}),"\n",(0,s.jsx)(e.h3,{id:"data-collection",children:"Data Collection"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Option 1: Simulation"})," (Chapter 7 datasets)"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"1000+ demonstrations"}),"\n",(0,s.jsx)(e.li,{children:"Perfect labels"}),"\n",(0,s.jsx)(e.li,{children:"Fast iteration"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Option 2: Real Robot"})," (teleoperation)"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"100-500 demonstrations"}),"\n",(0,s.jsx)(e.li,{children:"Higher fidelity"}),"\n",(0,s.jsx)(e.li,{children:"Slower but more realistic"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"data-format-rlds",children:"Data Format (RLDS)"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import tensorflow_datasets as tfds\n\n# Convert Chapter 7 data to RLDS\ndataset = {\n    "steps": [\n        {\n            "observation": {\n                "image": np.array(...),  # 224x224x3\n                "state": np.array(...),   # 12-DOF joints\n            },\n            "action": np.array(...),      # 12-DOF targets\n            "language_instruction": "Pick up the cup",\n            "is_terminal": False,\n        },\n        # ... more steps\n    ]\n}\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"3-fine-tuning-with-lora",children:"3. Fine-Tuning with LoRA"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"LoRA"})," (Low-Rank Adaptation): Efficient fine-tuning method"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Freezes base model (7B params)"}),"\n",(0,s.jsx)(e.li,{children:"Trains small adapter (8M params)"}),"\n",(0,s.jsx)(e.li,{children:"100x less memory, 10x faster"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"training-script",children:"Training Script"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from openvla import OpenVLA\nfrom peft import LoraConfig, get_peft_model\n\n# Load base model\nmodel = OpenVLA.from_pretrained("openvla/openvla-7b")\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,              # Rank\n    lora_alpha=32,\n    target_modules=["q_proj", "v_proj"],\n    lora_dropout=0.1,\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\n# Train\ntrainer = VLATrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    batch_size=8,\n    learning_rate=1e-4,\n    num_epochs=10,\n)\n\ntrainer.train()\nmodel.save_pretrained("humanoid_vla_lora")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"4-training-configuration",children:"4. Training Configuration"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Hardware Requirements"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"GPU: RTX 4090 (24GB VRAM) or A100"}),"\n",(0,s.jsx)(e.li,{children:"RAM: 64GB"}),"\n",(0,s.jsx)(e.li,{children:"Storage: 500GB for datasets"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training Time"})," (1000 demos):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"RTX 4090: 4-6 hours"}),"\n",(0,s.jsx)(e.li,{children:"A100: 2-3 hours"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Hyperparameters"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:"batch_size: 8\nlearning_rate: 1e-4\nlora_rank: 16\nepochs: 10\ngradient_accumulation: 4\nmixed_precision: fp16\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"5-evaluation-metrics",children:"5. Evaluation Metrics"}),"\n",(0,s.jsx)(e.h3,{id:"success-rate",children:"Success Rate"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'success_count = 0\nfor episode in test_set:\n    predicted_action = model.predict(image, instruction)\n    success = execute_action(predicted_action)\n    if success:\n        success_count += 1\n\nsuccess_rate = success_count / len(test_set)\nprint(f"Success Rate: {success_rate:.1%}")\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Target"}),": >70% success on held-out tasks"]}),"\n",(0,s.jsx)(e.h3,{id:"action-error",children:"Action Error"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'mse = np.mean((predicted_actions - ground_truth) ** 2)\nprint(f"Action MSE: {mse:.4f} radians")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"6-deployment",children:"6. Deployment"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Load fine-tuned model\nmodel = OpenVLA.from_pretrained(\n    "openvla/openvla-7b",\n    peft_model="humanoid_vla_lora"\n)\n\n# ROS 2 integration\nclass VLAControllerNode(Node):\n    def __init__(self):\n        self.model = model.to("cuda")\n        self.image_sub = self.create_subscription(...)\n        self.action_pub = self.create_publisher(...)\n\n    def image_callback(self, msg):\n        action = self.model.predict_action(\n            image=msg,\n            instruction=self.current_instruction\n        )\n        self.action_pub.publish(action)\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"7-hands-on-lab-fine-tune-on-grasping-4-hours",children:"7. Hands-On Lab: Fine-Tune on Grasping (4 hours)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Goal"}),": Fine-tune OpenVLA on Chapter 7 grasping dataset."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Convert synthetic data to RLDS format"}),"\n",(0,s.jsx)(e.li,{children:"Configure LoRA training"}),"\n",(0,s.jsx)(e.li,{children:"Train for 10 epochs (~2 hours on RTX 4090)"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate on 100 held-out images"}),"\n",(0,s.jsx)(e.li,{children:"Compare pre-trained vs fine-tuned success rates"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Validation"}),": Fine-tuned model achieves >70% success"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"8-end-of-chapter-project",children:"8. End-of-Chapter Project"}),"\n",(0,s.jsx)(e.p,{children:"Fine-tune VLA for humanoid dish-loading task."}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Collect 500 demonstrations (sim or real)"}),"\n",(0,s.jsx)(e.li,{children:"Fine-tune with LoRA"}),"\n",(0,s.jsx)(e.li,{children:"Achieve >60% success rate"}),"\n",(0,s.jsx)(e.li,{children:"Deploy to ROS 2 for real-time control"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Deliverables"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Fine-tuned model checkpoint"}),"\n",(0,s.jsx)(e.li,{children:"Evaluation report with metrics"}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 deployment package"}),"\n",(0,s.jsx)(e.li,{children:"Demo video"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"Fine-tuning VLAs with LoRA enables efficient adaptation to custom robots and tasks. With 500-1000 demonstrations, you can achieve 70%+ success on humanoid manipulation tasks."}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Next"}),": Chapter 15 covers multimodal reasoning for complex decision-making."]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var r=i(6540);const s={},t=r.createContext(s);function a(n){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(t.Provider,{value:e},n.children)}}}]);