"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[283],{5274:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"part2-simulation/isaac-sim-advanced","title":"Chapter 7: Isaac Sim Advanced - Domain Randomization, Synthetic Data, Replicator","description":"Learning Objectives","source":"@site/docs/part2-simulation/07-isaac-sim-advanced.md","sourceDirName":"part2-simulation","slug":"/part2-simulation/isaac-sim-advanced","permalink":"/robot_book/docs/part2-simulation/isaac-sim-advanced","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 6: NVIDIA Isaac Sim - Introduction and Setup","permalink":"/robot_book/docs/part2-simulation/isaac-sim-introduction"},"next":{"title":"Chapter 8: Simulation Benchmarking","permalink":"/robot_book/docs/part2-simulation/simulation-benchmarking"}}');var r=i(4848),t=i(8453);const a={},o="Chapter 7: Isaac Sim Advanced - Domain Randomization, Synthetic Data, Replicator",l={},d=[{value:"1. Domain Randomization for Sim-to-Real",id:"1-domain-randomization-for-sim-to-real",level:2},{value:"1.1 What is Domain Randomization?",id:"11-what-is-domain-randomization",level:3},{value:"1.2 Randomizing Visual Appearance: Textures, Colors, Lighting",id:"12-randomizing-visual-appearance-textures-colors-lighting",level:3},{value:"1.3 Randomizing Physics: Mass, Friction, Joint Damping",id:"13-randomizing-physics-mass-friction-joint-damping",level:3},{value:"1.4 Randomizing Sensor Noise",id:"14-randomizing-sensor-noise",level:3},{value:"2. Synthetic Data Generation",id:"2-synthetic-data-generation",level:2},{value:"2.1 Why Synthetic Data for Robotics?",id:"21-why-synthetic-data-for-robotics",level:3},{value:"2.2 Annotating Images: Bounding Boxes, Segmentation Masks, Depth Maps",id:"22-annotating-images-bounding-boxes-segmentation-masks-depth-maps",level:3},{value:"2.3 Isaac Sim&#39;s Built-In Annotation Tools",id:"23-isaac-sims-built-in-annotation-tools",level:3},{value:"3. Isaac Sim Replicator API",id:"3-isaac-sim-replicator-api",level:2},{value:"3.1 Scripting Procedural Scenes with Python",id:"31-scripting-procedural-scenes-with-python",level:3},{value:"3.2 Randomizing Object Placement and Poses",id:"32-randomizing-object-placement-and-poses",level:3},{value:"3.3 Generating Large-Scale Datasets (1000+ images)",id:"33-generating-large-scale-datasets-1000-images",level:3},{value:"3.4 Exporting Data in COCO, Pascal VOC, or Custom Formats",id:"34-exporting-data-in-coco-pascal-voc-or-custom-formats",level:3},{value:"4. Performance Profiling",id:"4-performance-profiling",level:2},{value:"4.1 Identifying Bottlenecks: Rendering vs. Physics vs. I/O",id:"41-identifying-bottlenecks-rendering-vs-physics-vs-io",level:3},{value:"4.2 Using Nsight Graphics for GPU Profiling",id:"42-using-nsight-graphics-for-gpu-profiling",level:3},{value:"4.3 Memory Usage Analysis",id:"43-memory-usage-analysis",level:3},{value:"5. Scene Optimization",id:"5-scene-optimization",level:2},{value:"5.1 Level of Detail (LOD) for Meshes",id:"51-level-of-detail-lod-for-meshes",level:3},{value:"5.2 Occlusion Culling and Frustum Culling",id:"52-occlusion-culling-and-frustum-culling",level:3},{value:"5.3 Shader Complexity Reduction",id:"53-shader-complexity-reduction",level:3},{value:"5.4 Physics Simplification: Convex Decomposition",id:"54-physics-simplification-convex-decomposition",level:3},{value:"6. Hands-On Lab: Randomized Grasping Dataset",id:"6-hands-on-lab-randomized-grasping-dataset",level:2},{value:"Lab Setup",id:"lab-setup",level:3},{value:"Step 1: Scene Creation (2 hours)",id:"step-1-scene-creation-2-hours",level:3},{value:"Step 2: Replicator Script (2 hours)",id:"step-2-replicator-script-2-hours",level:3},{value:"Step 3: Run and Verify (30 minutes)",id:"step-3-run-and-verify-30-minutes",level:3},{value:"Step 4: Convert to COCO (1 hour)",id:"step-4-convert-to-coco-1-hour",level:3},{value:"Step 5: Visualize Dataset (30 minutes)",id:"step-5-visualize-dataset-30-minutes",level:3},{value:"7. End-of-Chapter Project: Multi-Robot Assembly Dataset",id:"7-end-of-chapter-project-multi-robot-assembly-dataset",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"chapter-7-isaac-sim-advanced---domain-randomization-synthetic-data-replicator",children:"Chapter 7: Isaac Sim Advanced - Domain Randomization, Synthetic Data, Replicator"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Learning Objectives"})}),"\n",(0,r.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Explain domain randomization and its role in closing the sim-to-real gap"}),"\n",(0,r.jsx)(e.li,{children:"Randomize visual properties (textures, colors, lighting) using USD and Python API"}),"\n",(0,r.jsx)(e.li,{children:"Randomize physics parameters (mass, friction, joint damping) for robust policies"}),"\n",(0,r.jsx)(e.li,{children:"Generate synthetic datasets with automatic annotations (bounding boxes, segmentation, depth)"}),"\n",(0,r.jsx)(e.li,{children:"Use Isaac Sim Replicator API to create procedural scenes at scale"}),"\n",(0,r.jsx)(e.li,{children:"Profile Isaac Sim performance to identify bottlenecks (rendering vs. physics vs. I/O)"}),"\n",(0,r.jsx)(e.li,{children:"Optimize scenes for training (LOD, culling, physics simplification)"}),"\n",(0,r.jsx)(e.li,{children:"Generate 1000+ labeled images for vision-based manipulation tasks"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Prerequisites"}),": Chapter 6 (Isaac Sim basics), Python programming, basic computer vision"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Estimated Time"}),": 12-14 hours (6 hours reading, 6-8 hours hands-on lab)"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"1-domain-randomization-for-sim-to-real",children:"1. Domain Randomization for Sim-to-Real"}),"\n",(0,r.jsx)(e.h3,{id:"11-what-is-domain-randomization",children:"1.1 What is Domain Randomization?"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Domain randomization"})," is a technique to train robust robot policies by exposing them to diverse simulated environments, reducing overfitting to specific simulation artifacts."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"The Sim-to-Real Gap Problem"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simulation"}),": Perfect lighting, no sensor noise, known object poses"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Real World"}),": Variable lighting, sensor noise, uncertain object detection"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Result"}),": Policies trained in simulation fail on real hardware"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Domain Randomization Solution"}),":\nInstead of trying to make simulation perfectly match reality (impossible), randomize simulation to cover the space of possible realities."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Randomization Dimensions"}),":"]}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Category"}),(0,r.jsx)(e.th,{children:"Parameters"}),(0,r.jsx)(e.th,{children:"Example Range"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Visual"})}),(0,r.jsx)(e.td,{children:"Texture, color, lighting"}),(0,r.jsx)(e.td,{children:"10 textures \xd7 5 light intensities = 50 variations"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Physics"})}),(0,r.jsx)(e.td,{children:"Mass, friction, damping"}),(0,r.jsx)(e.td,{children:"Mass \xb1 20%, friction 0.5-1.5"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Sensor"})}),(0,r.jsx)(e.td,{children:"Camera noise, IMU drift"}),(0,r.jsx)(e.td,{children:"Gaussian noise \u03c3 = 0.01-0.05"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Geometry"})}),(0,r.jsx)(e.td,{children:"Object size, robot dimensions"}),(0,r.jsx)(e.td,{children:"Scale 0.9-1.1\xd7"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Dynamics"})}),(0,r.jsx)(e.td,{children:"External forces, delays"}),(0,r.jsx)(e.td,{children:"Wind 0-5 N, latency 10-50ms"})]})]})]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Example"}),": Training a grasping policy with domain randomization"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Without DR"}),": 95% success in sim, 30% on real robot (overfits to sim lighting)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"With DR"}),": 85% success in sim, 75% on real robot (robust to lighting variations)"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Key Papers"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"OpenAI (2018)"}),": Trained Dactyl hand to solve Rubik's cube using DR in simulation, deployed to real robot"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"NVIDIA (2021)"}),": Factory digital twin with DR achieves 90%+ sim-to-real transfer"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"12-randomizing-visual-appearance-textures-colors-lighting",children:"1.2 Randomizing Visual Appearance: Textures, Colors, Lighting"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Texture Randomization"}),":"]}),"\n",(0,r.jsx)(e.p,{children:"Isaac Sim supports dynamic texture replacement via USD Material API."}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Example"})," (Python script):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import UsdShade, Sdf\nimport omni.isaac.core.utils.prims as prim_utils\nimport random\n\ndef randomize_texture(prim_path: str, texture_paths: list):\n    """\n    Randomize texture on a prim.\n\n    Args:\n        prim_path: USD path to prim (e.g., "/World/Table")\n        texture_paths: List of texture file paths\n    """\n    prim = prim_utils.get_prim_at_path(prim_path)\n\n    # Get or create material\n    material_path = f"{prim_path}/Material"\n    material = UsdShade.Material.Get(stage, material_path)\n    if not material:\n        material = UsdShade.Material.Define(stage, material_path)\n\n    # Get shader\n    shader = UsdShade.Shader.Define(stage, f"{material_path}/Shader")\n    shader.CreateIdAttr("UsdPreviewSurface")\n\n    # Randomize diffuse texture\n    texture_path = random.choice(texture_paths)\n    diffuse_texture = UsdShade.Shader.Define(stage, f"{material_path}/DiffuseTexture")\n    diffuse_texture.CreateIdAttr("UsdUVTexture")\n    diffuse_texture.CreateInput("file", Sdf.ValueTypeNames.Asset).Set(texture_path)\n\n    # Connect texture to shader\n    shader.CreateInput("diffuseColor", Sdf.ValueTypeNames.Color3f).ConnectToSource(\n        diffuse_texture.ConnectableAPI(), "rgb"\n    )\n\n    # Bind material to prim\n    UsdShade.MaterialBindingAPI(prim).Bind(material)\n\n# Usage\ntexture_library = [\n    "omniverse://localhost/NVIDIA/Assets/Materials/Wood/WoodFloor_01.mdl",\n    "omniverse://localhost/NVIDIA/Assets/Materials/Metal/BrushedAluminum.mdl",\n    "omniverse://localhost/NVIDIA/Assets/Materials/Plastic/PlasticMatte.mdl",\n]\n\nrandomize_texture("/World/Table", texture_library)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Color Randomization"})," (simpler, faster):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import UsdGeom, Gf\nimport numpy as np\n\ndef randomize_color(prim_path: str):\n    """Randomize RGB color of a prim."""\n    geom = UsdGeom.Mesh.Get(stage, prim_path)\n\n    # Random RGB (0.2-0.9 to avoid pure black/white)\n    color = Gf.Vec3f(*np.random.uniform(0.2, 0.9, 3))\n    geom.GetDisplayColorAttr().Set([color])\n\n# Usage\nrandomize_color("/World/Cube")\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Lighting Randomization"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def randomize_lighting(light_path: str):\n    """Randomize intensity and color of light."""\n    from pxr import UsdLux\n\n    light = UsdLux.DistantLight.Get(stage, light_path)\n\n    # Randomize intensity (500-2000)\n    intensity = random.uniform(500, 2000)\n    light.GetIntensityAttr().Set(intensity)\n\n    # Randomize color temperature (3000K-6500K: warm to cool)\n    temp = random.uniform(3000, 6500)\n    light.GetColorTemperatureAttr().Set(temp)\n\n    # Randomize direction (sun angle)\n    angle_x = random.uniform(0, 90)\n    angle_z = random.uniform(0, 360)\n    # Apply rotation to light prim...\n\nrandomize_lighting("/World/Sun")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"13-randomizing-physics-mass-friction-joint-damping",children:"1.3 Randomizing Physics: Mass, Friction, Joint Damping"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Mass Randomization"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import UsdPhysics\n\ndef randomize_mass(prim_path: str, variance: float = 0.2):\n    """\n    Randomize mass of a rigid body by \xb1variance.\n\n    Args:\n        prim_path: USD path to rigid body\n        variance: Fractional variance (0.2 = \xb120%)\n    """\n    prim = prim_utils.get_prim_at_path(prim_path)\n    mass_api = UsdPhysics.MassAPI(prim)\n\n    # Get original mass\n    original_mass = mass_api.GetMassAttr().Get()\n\n    # Randomize (e.g., \xb120%)\n    scale = random.uniform(1 - variance, 1 + variance)\n    new_mass = original_mass * scale\n\n    mass_api.GetMassAttr().Set(new_mass)\n\n# Usage\nrandomize_mass("/World/Humanoid/base_link", variance=0.15)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Friction Randomization"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def randomize_friction(prim_path: str, mu_range: tuple = (0.5, 1.5)):\n    """Randomize Coulomb friction coefficient."""\n    prim = prim_utils.get_prim_at_path(prim_path)\n\n    # Get or create physics material\n    material_path = f"{prim_path}/PhysicsMaterial"\n    physics_material = UsdPhysics.MaterialAPI.Get(stage, material_path)\n    if not physics_material:\n        physics_material = UsdPhysics.MaterialAPI.Apply(prim)\n\n    # Randomize friction\n    mu = random.uniform(*mu_range)\n    physics_material.CreateStaticFrictionAttr(mu)\n    physics_material.CreateDynamicFrictionAttr(mu * 0.95)  # Dynamic < static\n\nrandomize_friction("/World/groundPlane/Plane", mu_range=(0.8, 1.2))\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Joint Damping Randomization"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import PhysxSchema\n\ndef randomize_joint_damping(joint_path: str, damping_range: tuple = (100, 1000)):\n    """Randomize joint damping coefficient."""\n    joint_prim = prim_utils.get_prim_at_path(joint_path)\n\n    # PhysX joint drive API\n    drive_api = UsdPhysics.DriveAPI.Get(joint_prim, "angular")\n    if not drive_api:\n        drive_api = UsdPhysics.DriveAPI.Apply(joint_prim, "angular")\n\n    # Randomize damping\n    damping = random.uniform(*damping_range)\n    drive_api.GetDampingAttr().Set(damping)\n\n# Usage (apply to all robot joints)\nfor joint_name in ["left_hip_joint", "left_knee_joint", "right_hip_joint", "right_knee_joint"]:\n    joint_path = f"/World/Humanoid/{joint_name}"\n    randomize_joint_damping(joint_path, damping_range=(300, 700))\n'})}),"\n",(0,r.jsx)(e.h3,{id:"14-randomizing-sensor-noise",children:"1.4 Randomizing Sensor Noise"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Camera Noise"})," (post-processing):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom PIL import Image\n\ndef add_camera_noise(image: np.ndarray, noise_std: float = 0.01):\n    """\n    Add Gaussian noise to RGB image.\n\n    Args:\n        image: HxWx3 numpy array (0-255 uint8)\n        noise_std: Standard deviation (fraction of 255)\n\n    Returns:\n        Noisy image\n    """\n    noise = np.random.normal(0, noise_std * 255, image.shape)\n    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)\n    return noisy_image\n\n# Apply after capturing from Isaac Sim camera\nrgb = get_camera_image()  # Returns numpy array\nnoisy_rgb = add_camera_noise(rgb, noise_std=0.02)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"IMU Noise"})," (in ActionGraph or Python):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def add_imu_noise(accel: np.ndarray, gyro: np.ndarray):\n    """\n    Add realistic IMU noise (white noise + bias).\n\n    Args:\n        accel: 3D acceleration vector (m/s\xb2)\n        gyro: 3D angular velocity vector (rad/s)\n\n    Returns:\n        Noisy accel, noisy gyro\n    """\n    # Noise parameters (based on MPU-6050 datasheet)\n    accel_noise_std = 0.017  # m/s\xb2\n    gyro_noise_std = 0.009  # rad/s (0.5 deg/s)\n    accel_bias = np.random.normal(0, 0.05, 3)\n    gyro_bias = np.random.normal(0, 0.001, 3)\n\n    # Add noise\n    noisy_accel = accel + np.random.normal(0, accel_noise_std, 3) + accel_bias\n    noisy_gyro = gyro + np.random.normal(0, gyro_noise_std, 3) + gyro_bias\n\n    return noisy_accel, noisy_gyro\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"2-synthetic-data-generation",children:"2. Synthetic Data Generation"}),"\n",(0,r.jsx)(e.h3,{id:"21-why-synthetic-data-for-robotics",children:"2.1 Why Synthetic Data for Robotics?"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Challenges with Real-World Data"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Expensive"}),": $1000+ per hour for human annotators"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Slow"}),": Weeks to label 10k images"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Limited Diversity"}),": Hard to capture rare scenarios (failures, edge cases)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Privacy"}),": Cannot collect data in private spaces"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Synthetic Data Benefits"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Free Annotations"}),": Segmentation, depth, normals auto-generated"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Scalable"}),": Generate 1M images overnight"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Diverse"}),": Procedural randomization covers infinite variations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safe"}),": Simulate dangerous scenarios (collisions, drops)"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Use Cases"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"VLA Training"}),": OpenVLA, RT-2, Octo require 100k-1M diverse demonstrations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Object Detection"}),": Train YOLOv8 on synthetic grasping scenes"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Depth Estimation"}),": Paired RGB-D data for monocular depth networks"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Segmentation"}),": Instance masks for part-based manipulation"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Challenges"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Domain Gap"}),': Synthetic images look "fake" (too perfect, wrong noise)']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Solution"}),": Domain randomization + GAN-based refinement"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"22-annotating-images-bounding-boxes-segmentation-masks-depth-maps",children:"2.2 Annotating Images: Bounding Boxes, Segmentation Masks, Depth Maps"}),"\n",(0,r.jsxs)(e.p,{children:["Isaac Sim provides ",(0,r.jsx)(e.strong,{children:"automatic annotations"})," via Synthetic Data Sensors (SyntheticData extension)."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Annotation Types"}),":"]}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Annotation"}),(0,r.jsx)(e.th,{children:"Description"}),(0,r.jsx)(e.th,{children:"Format"}),(0,r.jsx)(e.th,{children:"Use Case"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Bounding Box (2D)"})}),(0,r.jsx)(e.td,{children:"[x, y, w, h] rectangle"}),(0,r.jsx)(e.td,{children:"COCO JSON"}),(0,r.jsx)(e.td,{children:"Object detection"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Semantic Segmentation"})}),(0,r.jsx)(e.td,{children:"Per-pixel class ID"}),(0,r.jsx)(e.td,{children:"PNG (8-bit indexed)"}),(0,r.jsx)(e.td,{children:"Scene parsing"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Instance Segmentation"})}),(0,r.jsx)(e.td,{children:"Per-pixel instance ID"}),(0,r.jsx)(e.td,{children:"PNG (16-bit)"}),(0,r.jsx)(e.td,{children:"Part-based manipulation"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Depth"})}),(0,r.jsx)(e.td,{children:"Per-pixel distance (m)"}),(0,r.jsx)(e.td,{children:"NPY (float32)"}),(0,r.jsx)(e.td,{children:"3D reconstruction"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Normal"})}),(0,r.jsx)(e.td,{children:"Per-pixel surface normal"}),(0,r.jsx)(e.td,{children:"PNG (RGB as XYZ)"}),(0,r.jsx)(e.td,{children:"Surface estimation"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Pose (3D)"})}),(0,r.jsx)(e.td,{children:"6-DOF object pose"}),(0,r.jsx)(e.td,{children:"JSON (position + quaternion)"}),(0,r.jsx)(e.td,{children:"Grasping, tracking"})]})]})]}),"\n",(0,r.jsx)(e.h3,{id:"23-isaac-sims-built-in-annotation-tools",children:"2.3 Isaac Sim's Built-In Annotation Tools"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Enable Synthetic Data Extension"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from omni.isaac.core.utils.extensions import enable_extension\nenable_extension("omni.syntheticdata")\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Capture Annotated Data"})," (Python API):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Create render product (camera view)\nrender_product = rep.create.render_product("/World/Camera", (640, 480))\n\n# Attach annotators\nrgb_annotator = rep.AnnotatorRegistry.get_annotator("rgb")\nbbox_annotator = rep.AnnotatorRegistry.get_annotator("bounding_box_2d_tight")\nsem_seg_annotator = rep.AnnotatorRegistry.get_annotator("semantic_segmentation")\ndepth_annotator = rep.AnnotatorRegistry.get_annotator("distance_to_camera")\n\n# Attach to render product\nrgb_annotator.attach([render_product])\nbbox_annotator.attach([render_product])\nsem_seg_annotator.attach([render_product])\ndepth_annotator.attach([render_product])\n\n# Capture frame\nrep.orchestrator.step()\n\n# Get data\nrgb = rgb_annotator.get_data()  # (H, W, 4) RGBA uint8\nbboxes = bbox_annotator.get_data()  # List of {"semanticId": int, "x_min": float, ...}\nsem_seg = sem_seg_annotator.get_data()  # (H, W, 4) class IDs in R channel\ndepth = depth_annotator.get_data()  # (H, W) float32 depth in meters\n\nprint(f"RGB shape: {rgb[\'data\'].shape}")\nprint(f"Num bounding boxes: {len(bboxes[\'data\'])}")\nprint(f"Depth range: {depth[\'data\'].min():.2f}m to {depth[\'data\'].max():.2f}m")\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Semantic Segmentation Setup"})," (assign class IDs):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import Sdf\nimport omni.isaac.core.utils.semantics as semantics_utils\n\n# Assign semantic class to objects\nsemantics_utils.add_update_semantics(\n    prim=prim_utils.get_prim_at_path("/World/Table"),\n    semantic_label="table",\n    type_label="class"\n)\n\nsemantics_utils.add_update_semantics(\n    prim=prim_utils.get_prim_at_path("/World/Cube"),\n    semantic_label="object",\n    type_label="class"\n)\n\n# Isaac Sim will auto-assign class IDs:\n# - "table" \u2192 ID 1 (red in visualization)\n# - "object" \u2192 ID 2 (green in visualization)\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"3-isaac-sim-replicator-api",children:"3. Isaac Sim Replicator API"}),"\n",(0,r.jsx)(e.h3,{id:"31-scripting-procedural-scenes-with-python",children:"3.1 Scripting Procedural Scenes with Python"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Replicator"})," is Isaac Sim's framework for procedural scene generation at scale."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Core Concepts"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Randomizers"}),": Functions that modify scene (e.g., ",(0,r.jsx)(e.code,{children:"randomize_pose"}),")"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Triggers"}),": When to randomize (e.g., every frame, every N frames)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Writers"}),": How to export data (e.g., COCO JSON, custom format)"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Basic Replicator Script"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Define randomization function\nwith rep.new_layer():\n    # Create camera\n    camera = rep.create.camera(position=(2, 2, 1), look_at=(0, 0, 0.5))\n\n    # Create render product\n    render_product = rep.create.render_product(camera, (640, 480))\n\n    # Define what to randomize\n    def randomize_scene():\n        # Randomize cube position\n        cube = rep.get.prims(path_pattern="/World/Cube")\n        with cube:\n            rep.modify.pose(\n                position=rep.distribution.uniform((-0.5, -0.5, 0.5), (0.5, 0.5, 1.5)),\n                rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n            )\n\n        # Randomize lighting\n        light = rep.get.prims(path_pattern="/World/Sun")\n        with light:\n            rep.modify.attribute(\n                "inputs:intensity",\n                rep.distribution.uniform(500, 2000)\n            )\n\n        return True  # Trigger successful\n\n    # Register randomizer\n    rep.randomizer.register(randomize_scene)\n\n    # Trigger: randomize every frame\n    with rep.trigger.on_frame():\n        rep.randomizer.randomize_scene()\n\n    # Writer: save to disk\n    writer = rep.WriterRegistry.get("BasicWriter")\n    writer.initialize(\n        output_dir="_output_replicator",\n        rgb=True,\n        bounding_box_2d_tight=True,\n        semantic_segmentation=True,\n        distance_to_camera=True\n    )\n    writer.attach([render_product])\n\n# Run replicator for 100 frames (generates 100 images)\nrep.orchestrator.run()\nfor i in range(100):\n    rep.orchestrator.step()\n    print(f"Generated frame {i+1}/100")\n\nrep.orchestrator.stop()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"32-randomizing-object-placement-and-poses",children:"3.2 Randomizing Object Placement and Poses"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Scatter Objects on Table Surface"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Define table surface bounds\ntable_min = (-0.5, -0.3, 0.75)  # (x, y, z) min\ntable_max = (0.5, 0.3, 0.75)    # (x, y, z) max (same Z = tabletop)\n\n# Get all graspable objects (assumed to have semantic label "object")\nobjects = rep.get.prims(semantics=[("class", "object")])\n\ndef scatter_objects():\n    """Randomly place objects on table surface."""\n    with objects:\n        # Random XY position on table, fixed Z\n        rep.modify.pose(\n            position=rep.distribution.uniform(table_min, table_max),\n            rotation=rep.distribution.uniform((0, 0, 0), (0, 0, 360))  # Only rotate around Z\n        )\n    return True\n\nrep.randomizer.register(scatter_objects)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Ensure No Collisions"})," (advanced, physics-based):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def scatter_objects_physics():\n    """\n    Scatter objects and let them settle with physics.\n    Ensures no interpenetration.\n    """\n    from omni.isaac.core import World\n\n    # Randomize positions (above table)\n    with objects:\n        rep.modify.pose(\n            position=rep.distribution.uniform(\n                (table_min[0], table_min[1], 1.0),  # 0.25m above table\n                (table_max[0], table_max[1], 1.5)\n            ),\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n        )\n\n    # Run physics for 60 frames (1 second at 60Hz) to let objects settle\n    world = World.instance()\n    for _ in range(60):\n        world.step(render=False)  # Physics only, no rendering\n\n    return True\n'})}),"\n",(0,r.jsx)(e.h3,{id:"33-generating-large-scale-datasets-1000-images",children:"3.3 Generating Large-Scale Datasets (1000+ images)"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Production Script"})," (generate 10k images):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import omni.replicator.core as rep\nfrom omni.isaac.kit import SimulationApp\n\n# Headless mode for max throughput\nsimulation_app = SimulationApp({"headless": True})\n\n# ... (scene setup, randomizer definitions) ...\n\n# Writer configuration\nwriter = rep.WriterRegistry.get("BasicWriter")\nwriter.initialize(\n    output_dir="/data/grasping_dataset",\n    rgb=True,\n    bounding_box_2d_tight=True,\n    instance_segmentation=True,\n    semantic_segmentation=True,\n    distance_to_camera=True,\n    normals=True,\n    file_name_prefix="grasp"\n)\n\n# Generate 10,000 images\nNUM_FRAMES = 10000\nrep.orchestrator.run()\n\nfor i in range(NUM_FRAMES):\n    rep.orchestrator.step()\n\n    if (i + 1) % 100 == 0:\n        print(f"Progress: {i+1}/{NUM_FRAMES} ({(i+1)/NUM_FRAMES*100:.1f}%)")\n\nrep.orchestrator.stop()\nsimulation_app.close()\n\nprint(f"Dataset generated: {NUM_FRAMES} images in /data/grasping_dataset")\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Performance"}),": RTX 4090 generates ~100 images/second (headless, 640\xd7480 resolution)."]}),"\n",(0,r.jsx)(e.h3,{id:"34-exporting-data-in-coco-pascal-voc-or-custom-formats",children:"3.4 Exporting Data in COCO, Pascal VOC, or Custom Formats"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"COCO Format"})," (for object detection):"]}),"\n",(0,r.jsx)(e.p,{children:"Isaac Sim's BasicWriter outputs bounding boxes as JSON. Convert to COCO:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import json\nimport os\nfrom pathlib import Path\n\ndef convert_to_coco(output_dir: str):\n    """Convert BasicWriter output to COCO format."""\n\n    # COCO structure\n    coco_dataset = {\n        "images": [],\n        "annotations": [],\n        "categories": [\n            {"id": 1, "name": "cube"},\n            {"id": 2, "name": "cylinder"},\n            {"id": 3, "name": "sphere"}\n        ]\n    }\n\n    annotation_id = 0\n\n    # Read BasicWriter bbox JSON files\n    bbox_dir = Path(output_dir) / "bounding_box_2d_tight"\n    for i, bbox_file in enumerate(sorted(bbox_dir.glob("*.json"))):\n        with open(bbox_file) as f:\n            data = json.load(f)\n\n        # Add image entry\n        image_id = i\n        coco_dataset["images"].append({\n            "id": image_id,\n            "file_name": f"rgb_{i:04d}.png",\n            "width": 640,\n            "height": 480\n        })\n\n        # Add annotations\n        for bbox in data["data"]:\n            semantic_id = bbox["semanticId"]\n            x_min = bbox["x_min"]\n            y_min = bbox["y_min"]\n            x_max = bbox["x_max"]\n            y_max = bbox["y_max"]\n\n            width = x_max - x_min\n            height = y_max - y_min\n            area = width * height\n\n            coco_dataset["annotations"].append({\n                "id": annotation_id,\n                "image_id": image_id,\n                "category_id": semantic_id,\n                "bbox": [x_min, y_min, width, height],  # COCO format: [x, y, w, h]\n                "area": area,\n                "iscrowd": 0\n            })\n            annotation_id += 1\n\n    # Save COCO JSON\n    with open(os.path.join(output_dir, "annotations.json"), "w") as f:\n        json.dump(coco_dataset, f, indent=2)\n\n    print(f"COCO dataset created: {len(coco_dataset[\'images\'])} images, {len(coco_dataset[\'annotations\'])} annotations")\n\n# Usage\nconvert_to_coco("/data/grasping_dataset")\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Custom Writer"})," (for specialized formats):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import omni.replicator.core as rep\n\nclass CustomWriter(rep.Writer):\n    """Custom writer for specific data format."""\n\n    def __init__(self, output_dir: str):\n        self.output_dir = output_dir\n        self.frame_id = 0\n\n    def write(self, data: dict):\n        """\n        Called for each frame.\n\n        Args:\n            data: Dict with keys: "rgb", "bounding_box_2d_tight", etc.\n        """\n        # Extract RGB\n        rgb = data["rgb"]["data"]  # (H, W, 4) RGBA\n\n        # Extract bounding boxes\n        bboxes = data["bounding_box_2d_tight"]["data"]\n\n        # Custom format: save RGB as PNG, bboxes as TXT\n        from PIL import Image\n        Image.fromarray(rgb[:, :, :3]).save(f"{self.output_dir}/frame_{self.frame_id:05d}.png")\n\n        with open(f"{self.output_dir}/frame_{self.frame_id:05d}.txt", "w") as f:\n            for bbox in bboxes:\n                # Format: class_id x_center y_center width height (YOLO format)\n                class_id = bbox["semanticId"]\n                x_center = (bbox["x_min"] + bbox["x_max"]) / 2 / 640  # Normalize\n                y_center = (bbox["y_min"] + bbox["y_max"]) / 2 / 480\n                width = (bbox["x_max"] - bbox["x_min"]) / 640\n                height = (bbox["y_max"] - bbox["y_min"]) / 480\n                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n")\n\n        self.frame_id += 1\n\n# Register and use custom writer\nrep.WriterRegistry.register(CustomWriter)\nwriter = CustomWriter(output_dir="/data/custom_dataset")\nwriter.attach([render_product])\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"4-performance-profiling",children:"4. Performance Profiling"}),"\n",(0,r.jsx)(e.h3,{id:"41-identifying-bottlenecks-rendering-vs-physics-vs-io",children:"4.1 Identifying Bottlenecks: Rendering vs. Physics vs. I/O"}),"\n",(0,r.jsx)(e.p,{children:"Isaac Sim's profiler shows time breakdown per frame."}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Enable Profiler"}),":"]}),"\n",(0,r.jsxs)(e.p,{children:["Menu: ",(0,r.jsx)(e.code,{children:"Window"})," \u2192 ",(0,r.jsx)(e.code,{children:"Profiler"})]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Key Metrics"}),":"]}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Metric"}),(0,r.jsx)(e.th,{children:"Description"}),(0,r.jsx)(e.th,{children:"Target"}),(0,r.jsx)(e.th,{children:"Bottleneck If..."})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Total Frame Time"})}),(0,r.jsx)(e.td,{children:"ms per frame"}),(0,r.jsx)(e.td,{children:"< 16.7ms (60 FPS)"}),(0,r.jsx)(e.td,{children:"> 33ms (< 30 FPS)"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Physics Time"})}),(0,r.jsx)(e.td,{children:"PhysX solve + collision"}),(0,r.jsx)(e.td,{children:"< 5ms"}),(0,r.jsx)(e.td,{children:"> 10ms \u2192 Simplify physics"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Render Time"})}),(0,r.jsx)(e.td,{children:"RTX ray tracing"}),(0,r.jsx)(e.td,{children:"< 10ms"}),(0,r.jsx)(e.td,{children:"> 20ms \u2192 Reduce resolution"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Python Time"})}),(0,r.jsx)(e.td,{children:"Script execution"}),(0,r.jsx)(e.td,{children:"< 1ms"}),(0,r.jsx)(e.td,{children:"> 5ms \u2192 Optimize Python code"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"I/O Time"})}),(0,r.jsx)(e.td,{children:"Disk writes"}),(0,r.jsx)(e.td,{children:"< 1ms"}),(0,r.jsx)(e.td,{children:"> 10ms \u2192 Use SSD, batch writes"})]})]})]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Interpret Profiler"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Frame Time: 45ms (22 FPS) \u2190 TOO SLOW\n\u251c\u2500 Physics: 8ms (18%)\n\u251c\u2500 Rendering: 30ms (67%) \u2190 BOTTLENECK!\n\u251c\u2500 Python: 5ms (11%)\n\u2514\u2500 I/O: 2ms (4%)\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Solution"}),": Reduce rendering load (lower resolution, disable RTX, headless mode)."]}),"\n",(0,r.jsx)(e.h3,{id:"42-using-nsight-graphics-for-gpu-profiling",children:"4.2 Using Nsight Graphics for GPU Profiling"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Nsight Graphics"})," is NVIDIA's GPU profiler for graphics applications."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Install"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Download from NVIDIA website\nwget https://developer.nvidia.com/downloads/assets/tools/secure/nsight-graphics/2024_1/nsight-graphics-linux-x64-2024.1.0.24068.deb\n\nsudo dpkg -i nsight-graphics-linux-x64-2024.1.0.24068.deb\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Profile Isaac Sim"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Launch Nsight Graphics: ",(0,r.jsx)(e.code,{children:"nsight-gfx"})]}),"\n",(0,r.jsx)(e.li,{children:"File \u2192 Connect"}),"\n",(0,r.jsxs)(e.li,{children:["Application: ",(0,r.jsx)(e.code,{children:"~/.local/share/ov/pkg/isaac-sim-4.2.0/isaac-sim.sh"})]}),"\n",(0,r.jsx)(e.li,{children:"Start profiling"}),"\n",(0,r.jsx)(e.li,{children:"In Isaac Sim: Play simulation for 10 seconds"}),"\n",(0,r.jsx)(e.li,{children:"Nsight Graphics: Capture frame"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Analysis"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPU Timeline"}),": See which shaders take longest"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Memory Usage"}),": Check VRAM allocation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Bottleneck"}),": Identify fragment shader vs. ray tracing vs. compute"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Common GPU Bottlenecks"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Fragment Shader"}),": Too many pixels (reduce resolution)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Ray Tracing"}),": Complex materials (simplify shaders)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Compute"}),": PhysX on GPU (reduce num_robots or simplify collisions)"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"43-memory-usage-analysis",children:"4.3 Memory Usage Analysis"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Monitor VRAM"})," (command line):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"watch -n 0.5 nvidia-smi\n\n# Output:\n# GPU  Name        Temp  Memory-Usage\n# 0    RTX 4080    65C   12GB / 16GB  \u2190 75% usage, still OK\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Python API"})," (in Isaac Sim script):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\nmem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n\nprint(f"VRAM Used: {mem_info.used / 1e9:.2f} GB")\nprint(f"VRAM Total: {mem_info.total / 1e9:.2f} GB")\nprint(f"VRAM Utilization: {mem_info.used / mem_info.total * 100:.1f}%")\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Reduce VRAM Usage"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Enable Instancing"}),": Share meshes across robots"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'enable_extension("omni.physx.flatcache")\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Reduce Texture Resolution"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# In material, set maxTextureSize\nmaterial.CreateInput("maxTextureSize", Sdf.ValueTypeNames.Int).Set(512)  # Was 2048\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Unload Unused Assets"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import Usd\nstage = omni.usd.get_context().get_stage()\nstage.Unload("/World/UnusedModel")  # Free VRAM\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"5-scene-optimization",children:"5. Scene Optimization"}),"\n",(0,r.jsx)(e.h3,{id:"51-level-of-detail-lod-for-meshes",children:"5.1 Level of Detail (LOD) for Meshes"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"LOD"})," reduces polygon count for distant objects."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Create LOD Manually"})," (Blender workflow):"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:["Export high-poly mesh: ",(0,r.jsx)(e.code,{children:"humanoid_high.obj"})," (100k triangles)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"Decimate in Blender:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Select mesh \u2192 Modifiers \u2192 Decimate"}),"\n",(0,r.jsxs)(e.li,{children:["Ratio: 0.5 (50k triangles) \u2192 Export as ",(0,r.jsx)(e.code,{children:"humanoid_mid.obj"})]}),"\n",(0,r.jsxs)(e.li,{children:["Ratio: 0.1 (10k triangles) \u2192 Export as ",(0,r.jsx)(e.code,{children:"humanoid_low.obj"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:"In Isaac Sim, swap meshes based on distance:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def update_lod(camera_pos, object_pos):\n    """Swap mesh based on distance to camera."""\n    distance = np.linalg.norm(camera_pos - object_pos)\n\n    if distance < 5:\n        mesh_path = "humanoid_high.obj"\n    elif distance < 20:\n        mesh_path = "humanoid_mid.obj"\n    else:\n        mesh_path = "humanoid_low.obj"\n\n    # Update mesh reference\n    # (code omitted for brevity, use USD API to swap mesh)\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"52-occlusion-culling-and-frustum-culling",children:"5.2 Occlusion Culling and Frustum Culling"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Frustum Culling"})," (automatic in Isaac Sim):"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Objects outside camera view are not rendered"}),"\n",(0,r.jsx)(e.li,{children:"No action needed (enabled by default)"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Occlusion Culling"})," (objects hidden behind others):"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Not automatic in Isaac Sim (ray tracing handles this)"}),"\n",(0,r.jsx)(e.li,{children:"For rasterization: Manually disable rendering for hidden objects"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Example"})," (disable robots not in camera view):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from pxr import Usd\n\ndef cull_out_of_view_robots(camera_prim_path: str, robot_paths: list):\n    """Disable rendering for robots outside camera frustum."""\n    camera_prim = prim_utils.get_prim_at_path(camera_prim_path)\n    # ... (compute camera frustum, test bounding boxes)\n\n    for robot_path in robot_paths:\n        if not in_frustum(robot_path):\n            robot_prim = prim_utils.get_prim_at_path(robot_path)\n            imageable = UsdGeom.Imageable(robot_prim)\n            imageable.MakeInvisible()  # Skip rendering\n'})}),"\n",(0,r.jsx)(e.h3,{id:"53-shader-complexity-reduction",children:"5.3 Shader Complexity Reduction"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Disable Expensive Effects"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Disable ambient occlusion (expensive)\nimport carb.settings\nsettings = carb.settings.get_settings()\nsettings.set("/rtx/post/aa/op", 0)  # Disable anti-aliasing\nsettings.set("/rtx/ambientOcclusion/enabled", False)\nsettings.set("/rtx/reflections/enabled", False)  # Disable reflections\nsettings.set("/rtx/translucency/enabled", False)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Use Simpler Materials"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Replace PBR material with basic diffuse\nfrom pxr import UsdShade, Sdf\n\ndef simplify_material(prim_path: str):\n    """Replace complex material with simple diffuse."""\n    material = UsdShade.Material.Get(stage, f"{prim_path}/Material")\n    shader = UsdShade.Shader.Get(stage, f"{prim_path}/Material/Shader")\n\n    # Clear all inputs (removes metallic, roughness, etc.)\n    for input_name in shader.GetInputs():\n        shader.GetInput(input_name).ClearConnections()\n\n    # Keep only diffuse color\n    shader.CreateInput("diffuseColor", Sdf.ValueTypeNames.Color3f).Set((0.8, 0.8, 0.8))\n'})}),"\n",(0,r.jsx)(e.h3,{id:"54-physics-simplification-convex-decomposition",children:"5.4 Physics Simplification: Convex Decomposition"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Problem"}),": Complex concave meshes require many collision primitives (slow)."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Solution"}),": Approximate with convex hulls."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"URDF Importer"})," (automatic convex decomposition):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from omni.isaac.urdf import _urdf\n\nimport_config = _urdf.ImportConfig()\nimport_config.convex_decomp = True  # Enable V-HACD decomposition\nimport_config.convex_decomp_params = {\n    "max_hulls": 16,  # Max convex hulls per mesh (fewer = faster)\n    "resolution": 100000  # Voxel resolution (lower = faster, less accurate)\n}\n\nurdf_interface.parse_urdf(urdf_path, "/World/Robot", import_config)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Manual Replacement"})," (use primitives):"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Replace complex foot mesh with simple box\nfoot_collision_path = "/World/Humanoid/left_foot/collisions"\nprim_utils.delete_prim(foot_collision_path)\n\n# Create box collision\nfrom pxr import UsdGeom, UsdPhysics\ncollision_prim = UsdGeom.Cube.Define(stage, foot_collision_path)\ncollision_prim.GetSizeAttr().Set(0.15)  # 15cm box\nUsdPhysics.CollisionAPI.Apply(collision_prim.GetPrim())\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"6-hands-on-lab-randomized-grasping-dataset",children:"6. Hands-On Lab: Randomized Grasping Dataset"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Generate 1000 labeled images of humanoid grasping random objects."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Time"}),": 6-8 hours"]}),"\n",(0,r.jsx)(e.h3,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Prerequisites"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Isaac Sim 2024.2 installed"}),"\n",(0,r.jsx)(e.li,{children:"Chapter 6 humanoid scene"}),"\n",(0,r.jsx)(e.li,{children:"10-20 graspable object meshes (cubes, cylinders, spheres)"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"step-1-scene-creation-2-hours",children:"Step 1: Scene Creation (2 hours)"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Launch Isaac Sim"}),"\n",(0,r.jsxs)(e.li,{children:["Create scene:","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Ground plane"}),"\n",(0,r.jsx)(e.li,{children:"Table (0.75m height, 1m \xd7 0.6m surface)"}),"\n",(0,r.jsx)(e.li,{children:"Humanoid positioned 0.5m from table"}),"\n",(0,r.jsx)(e.li,{children:"20 small objects (0.05-0.15m size) on table"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["Add camera:","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Position: (1.5, 0, 1.2)"}),"\n",(0,r.jsx)(e.li,{children:"Look at: table center"}),"\n",(0,r.jsx)(e.li,{children:"Resolution: 640 \xd7 480"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["Assign semantic labels:","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'Table: "table"'}),"\n",(0,r.jsx)(e.li,{children:'Objects: "cube", "cylinder", "sphere" (by geometry)'}),"\n",(0,r.jsx)(e.li,{children:'Humanoid: "robot"'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"step-2-replicator-script-2-hours",children:"Step 2: Replicator Script (2 hours)"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"File"}),": ",(0,r.jsx)(e.code,{children:"generate_grasping_dataset.py"})]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import omni.replicator.core as rep\nfrom omni.isaac.kit import SimulationApp\n\n# Launch headless\nsimulation_app = SimulationApp({"headless": True})\n\nimport numpy as np\n\n# ... (load scene from Step 1) ...\n\n# Camera and render product\ncamera = rep.create.camera(position=(1.5, 0, 1.2), look_at=(0, 0, 0.8))\nrender_product = rep.create.render_product(camera, (640, 480))\n\n# Get objects to randomize\nobjects = rep.get.prims(semantics=[("class", "cube"), ("class", "cylinder"), ("class", "sphere")])\ntable = rep.get.prims(path_pattern="/World/Table")\nlights = rep.get.prims(semantics=[("class", "light")])\n\ndef randomize_grasping_scene():\n    """Randomize object poses, textures, and lighting."""\n\n    # Scatter objects on table (physics-based)\n    with objects:\n        rep.modify.pose(\n            position=rep.distribution.uniform((-0.4, -0.25, 0.9), (0.4, 0.25, 1.2)),\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n        )\n\n    # Let objects settle with physics\n    from omni.isaac.core import World\n    world = World.instance()\n    for _ in range(120):  # 2 seconds at 60Hz\n        world.step(render=False)\n\n    # Randomize table texture\n    with table:\n        rep.randomizer.texture(\n            textures=[\n                "omniverse://localhost/NVIDIA/Assets/Materials/Wood/WoodFloor_01.mdl",\n                "omniverse://localhost/NVIDIA/Assets/Materials/Metal/BrushedAluminum.mdl",\n            ]\n        )\n\n    # Randomize lighting\n    with lights:\n        rep.modify.attribute("inputs:intensity", rep.distribution.uniform(500, 2500))\n        rep.modify.attribute("inputs:color", rep.distribution.uniform((0.8, 0.8, 0.8), (1.0, 1.0, 1.0)))\n\n    return True\n\nrep.randomizer.register(randomize_grasping_scene)\n\n# Trigger\nwith rep.trigger.on_frame():\n    rep.randomizer.randomize_grasping_scene()\n\n# Writer\nwriter = rep.WriterRegistry.get("BasicWriter")\nwriter.initialize(\n    output_dir="/data/grasping_1k",\n    rgb=True,\n    bounding_box_2d_tight=True,\n    instance_segmentation=True,\n    semantic_segmentation=True,\n    distance_to_camera=True\n)\nwriter.attach([render_product])\n\n# Generate 1000 frames\nNUM_FRAMES = 1000\nrep.orchestrator.run()\n\nfor i in range(NUM_FRAMES):\n    rep.orchestrator.step()\n    if (i + 1) % 100 == 0:\n        print(f"Generated {i+1}/{NUM_FRAMES} images")\n\nrep.orchestrator.stop()\nsimulation_app.close()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"step-3-run-and-verify-30-minutes",children:"Step 3: Run and Verify (30 minutes)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"cd ~/.local/share/ov/pkg/isaac-sim-4.2.0\n./python.sh ~/path/to/generate_grasping_dataset.py\n\n# Wait ~30-60 minutes (RTX 4080: ~30 images/second)\n# Output: /data/grasping_1k/ with 1000 images + annotations\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Verify Dataset"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ls /data/grasping_1k/\n# rgb/               # 1000 PNG images\n# bounding_box_2d_tight/  # 1000 JSON files\n# semantic_segmentation/  # 1000 PNG masks\n# distance_to_camera/     # 1000 NPY depth maps\n"})}),"\n",(0,r.jsx)(e.h3,{id:"step-4-convert-to-coco-1-hour",children:"Step 4: Convert to COCO (1 hour)"}),"\n",(0,r.jsx)(e.p,{children:"Run conversion script from Section 3.4."}),"\n",(0,r.jsx)(e.h3,{id:"step-5-visualize-dataset-30-minutes",children:"Step 5: Visualize Dataset (30 minutes)"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Script"}),": ",(0,r.jsx)(e.code,{children:"visualize_dataset.py"})]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import json\nimport cv2\nimport numpy as np\nfrom pathlib import Path\n\ndataset_dir = Path("/data/grasping_1k")\n\n# Load COCO annotations\nwith open(dataset_dir / "annotations.json") as f:\n    coco = json.load(f)\n\n# Visualize first 10 images\nfor i in range(10):\n    # Load RGB\n    img_path = dataset_dir / "rgb" / f"rgb_{i:04d}.png"\n    img = cv2.imread(str(img_path))\n\n    # Find annotations for this image\n    img_id = coco["images"][i]["id"]\n    annotations = [ann for ann in coco["annotations"] if ann["image_id"] == img_id]\n\n    # Draw bounding boxes\n    for ann in annotations:\n        x, y, w, h = ann["bbox"]\n        cat_id = ann["category_id"]\n        cat_name = next(cat["name"] for cat in coco["categories"] if cat["id"] == cat_id)\n\n        cv2.rectangle(img, (int(x), int(y)), (int(x+w), int(y+h)), (0, 255, 0), 2)\n        cv2.putText(img, cat_name, (int(x), int(y)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Display\n    cv2.imshow(f"Image {i}", img)\n    cv2.waitKey(1000)  # 1 second per image\n\ncv2.destroyAllWindows()\n'})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"7-end-of-chapter-project-multi-robot-assembly-dataset",children:"7. End-of-Chapter Project: Multi-Robot Assembly Dataset"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Generate dataset of 2 humanoids collaboratively assembling objects (sim-to-real transfer for multi-agent manipulation)."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Scene"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"2 humanoids facing each other across table"}),"\n",(0,r.jsx)(e.li,{children:"20 assembly parts (blocks, connectors)"}),"\n",(0,r.jsx)(e.li,{children:"Random part placements"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Randomization"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Robot arm joint positions (\xb120\xb0)"}),"\n",(0,r.jsx)(e.li,{children:"Part colors (10 color variations)"}),"\n",(0,r.jsx)(e.li,{children:"Lighting (day/night simulation)"}),"\n",(0,r.jsx)(e.li,{children:"Camera viewpoint (5 viewpoints around table)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Annotations"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"2D bounding boxes for all parts"}),"\n",(0,r.jsx)(e.li,{children:"6-DOF poses for parts (for grasping)"}),"\n",(0,r.jsx)(e.li,{children:"Robot joint states (for imitation learning)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Dataset Size"}),": 5000 images (1 hour generation on RTX 4090)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Output"}),": COCO JSON + custom robot state JSON"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Starter Code"}),": Extend ",(0,r.jsx)(e.code,{children:"generate_grasping_dataset.py"})," with second humanoid and 6-DOF pose annotations."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Domain Randomization"}),": Randomize visual, physics, and sensor properties to close sim-to-real gap"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Synthetic Data"}),": Generate labeled datasets (bboxes, segmentation, depth) automatically"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Replicator API"}),": Procedural scene generation at scale (1000+ images in minutes)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Performance Profiling"}),": Identify bottlenecks (rendering vs. physics vs. I/O)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Scene Optimization"}),": LOD, culling, shader simplification, physics approximation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Hands-On Skills"}),": Generated 1000-image grasping dataset with full annotations"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Domain randomization is ",(0,r.jsx)(e.strong,{children:"essential"})," for sim-to-real transfer (OpenAI Dactyl, NVIDIA factory twins)"]}),"\n",(0,r.jsxs)(e.li,{children:["Isaac Sim's Replicator API enables ",(0,r.jsx)(e.strong,{children:"industrial-scale"})," synthetic data generation"]}),"\n",(0,r.jsxs)(e.li,{children:["Performance tuning is ",(0,r.jsx)(e.strong,{children:"critical"}),": 10\xd7 speedup possible with proper optimization"]}),"\n",(0,r.jsxs)(e.li,{children:["Always profile ",(0,r.jsx)(e.strong,{children:"before"})," optimizing: Don't guess bottlenecks"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Next Chapter Preview"}),": Chapter 8 covers Simulation Benchmarking\u2014comparing Gazebo, Isaac Sim, MuJoCo across fidelity, performance, and sim-to-real metrics."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Domain Randomization"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["OpenAI Dactyl Paper: ",(0,r.jsx)(e.a,{href:"https://arxiv.org/abs/1808.00177",children:"https://arxiv.org/abs/1808.00177"})]}),"\n",(0,r.jsxs)(e.li,{children:["Domain Randomization Survey: ",(0,r.jsx)(e.a,{href:"https://arxiv.org/abs/1910.07113",children:"https://arxiv.org/abs/1910.07113"})]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Synthetic Data"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["NVIDIA Synthetic Data Blog: ",(0,r.jsx)(e.a,{href:"https://developer.nvidia.com/blog/synthetic-data-generation/",children:"https://developer.nvidia.com/blog/synthetic-data-generation/"})]}),"\n",(0,r.jsxs)(e.li,{children:["COCO Dataset Format: ",(0,r.jsx)(e.a,{href:"https://cocodataset.org/#format-data",children:"https://cocodataset.org/#format-data"})]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Replicator API"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Isaac Sim Replicator Docs: ",(0,r.jsx)(e.a,{href:"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html",children:"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html"})]}),"\n",(0,r.jsxs)(e.li,{children:["Replicator Examples: ",(0,r.jsx)(e.a,{href:"https://github.com/NVIDIA-Omniverse/synthetic-data-examples",children:"https://github.com/NVIDIA-Omniverse/synthetic-data-examples"})]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Nsight Graphics: ",(0,r.jsx)(e.a,{href:"https://developer.nvidia.com/nsight-graphics",children:"https://developer.nvidia.com/nsight-graphics"})]}),"\n",(0,r.jsxs)(e.li,{children:["USD Performance Best Practices: ",(0,r.jsx)(e.a,{href:"https://graphics.pixar.com/usd/docs/USD-Glossary.html",children:"https://graphics.pixar.com/usd/docs/USD-Glossary.html"})]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Research Papers"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["RT-2 (Google): ",(0,r.jsx)(e.a,{href:"https://robotics-transformer2.github.io/",children:"https://robotics-transformer2.github.io/"})]}),"\n",(0,r.jsxs)(e.li,{children:["OpenVLA (Stanford): ",(0,r.jsx)(e.a,{href:"https://openvla.github.io/",children:"https://openvla.github.io/"})]}),"\n",(0,r.jsxs)(e.li,{children:["Isaac Gym (NVIDIA): ",(0,r.jsx)(e.a,{href:"https://arxiv.org/abs/2108.10470",children:"https://arxiv.org/abs/2108.10470"})]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"End of Chapter 7"})})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(n){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),s.createElement(t.Provider,{value:e},n.children)}}}]);