"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[196],{819:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"part4-vla/vla-architecture","title":"Chapter 13: VLA Architecture Fundamentals","description":"Learning Objectives","source":"@site/docs/part4-vla/13-vla-architecture.md","sourceDirName":"part4-vla","slug":"/part4-vla/vla-architecture","permalink":"/robot_book/docs/part4-vla/vla-architecture","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":13,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 12: Containerization & CI/CD","permalink":"/robot_book/docs/part3-perception/containerization-cicd"},"next":{"title":"Chapter 14: OpenVLA Fine-Tuning","permalink":"/robot_book/docs/part4-vla/openvla-finetuning"}}');var t=i(4848),s=i(8453);const a={},o="Chapter 13: VLA Architecture Fundamentals",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. What are VLAs?",id:"1-what-are-vlas",level:2},{value:"2. VLA vs Traditional Approaches",id:"2-vla-vs-traditional-approaches",level:2},{value:"3. Architecture Components",id:"3-architecture-components",level:2},{value:"Vision Encoder (CLIP/DINOv2)",id:"vision-encoder-clipdinov2",level:3},{value:"Language Model (LLaMA/GPT)",id:"language-model-llamagpt",level:3},{value:"Action Decoder",id:"action-decoder",level:3},{value:"4. OpenVLA Installation",id:"4-openvla-installation",level:2},{value:"5. Inference Example",id:"5-inference-example",level:2},{value:"6. Training Data Format",id:"6-training-data-format",level:2},{value:"7. Hands-On Lab: VLA Inference (2 hours)",id:"7-hands-on-lab-vla-inference-2-hours",level:2},{value:"8. End-of-Chapter Project",id:"8-end-of-chapter-project",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-13-vla-architecture-fundamentals",children:"Chapter 13: VLA Architecture Fundamentals"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Understand"})," Vision-Language-Action (VLA) model architecture"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Compare"})," VLA vs traditional RL approaches"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Identify"})," key components: vision encoder, language model, action decoder"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Install"})," OpenVLA framework"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Run"})," pre-trained VLA models for inference"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"1-what-are-vlas",children:"1. What are VLAs?"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Vision-Language-Action Models"})," combine:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision"}),": Process camera images (RGB/RGBD)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language"}),": Understand natural language instructions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action"}),": Output robot control commands"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example Task"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'Input: Image + "Pick up the red cup"'}),"\n",(0,t.jsx)(e.li,{children:"Output: 7-DOF arm joint positions"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Key Insight"}),": VLAs learn generalizable policies from large-scale robot data, unlike traditional RL which requires task-specific training."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"2-vla-vs-traditional-approaches",children:"2. VLA vs Traditional Approaches"}),"\n",(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:"Approach"}),(0,t.jsx)(e.th,{children:"Training Data"}),(0,t.jsx)(e.th,{children:"Generalization"}),(0,t.jsx)(e.th,{children:"Example"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"Traditional RL"})}),(0,t.jsx)(e.td,{children:"Task-specific (1K demos)"}),(0,t.jsx)(e.td,{children:"Limited"}),(0,t.jsx)(e.td,{children:"Pick red cup only"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:"VLA"})}),(0,t.jsx)(e.td,{children:"Multi-task (1M demos)"}),(0,t.jsx)(e.td,{children:"Broad"}),(0,t.jsx)(e.td,{children:"Pick any object by name"})]})]})]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"VLA Advantages"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Zero-shot transfer to new objects"}),"\n",(0,t.jsx)(e.li,{children:"Language-conditioned behavior"}),"\n",(0,t.jsx)(e.li,{children:"Scales with data (like LLMs)"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"3-architecture-components",children:"3. Architecture Components"}),"\n",(0,t.jsx)(e.h3,{id:"vision-encoder-clipdinov2",children:"Vision Encoder (CLIP/DINOv2)"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Input: 224\xd7224 RGB image\n\u2193\nPretrained Vision Transformer (ViT)\n\u2193\nOutput: 768-dim embedding\n"})}),"\n",(0,t.jsx)(e.h3,{id:"language-model-llamagpt",children:"Language Model (LLaMA/GPT)"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:'Input: "Pick up the cup"\n\u2193\nTransformer decoder\n\u2193\nOutput: Text embeddings aligned with vision\n'})}),"\n",(0,t.jsx)(e.h3,{id:"action-decoder",children:"Action Decoder"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Input: Vision + language embeddings\n\u2193\nMLP layers\n\u2193\nOutput: Robot actions (joint positions or velocities)\n"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"End-to-End Training"}),": All components fine-tuned jointly on robot data."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"4-openvla-installation",children:"4. OpenVLA Installation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Clone OpenVLA\ngit clone https://github.com/openvla/openvla.git\ncd openvla\n\n# Install dependencies\npip install -e .\npip install torch torchvision transformers\n\n# Download pre-trained model (7B params)\nhuggingface-cli download openvla/openvla-7b --local-dir models/openvla-7b\n"})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"5-inference-example",children:"5. Inference Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from openvla import OpenVLA\nimport torch\nfrom PIL import Image\n\n# Load model\nmodel = OpenVLA.from_pretrained("openvla/openvla-7b")\nmodel = model.to("cuda")\n\n# Load image\nimage = Image.open("robot_view.jpg")\n\n# Language instruction\ninstruction = "Pick up the red mug"\n\n# Get action\nwith torch.no_grad():\n    action = model.predict_action(\n        image=image,\n        instruction=instruction\n    )\n\nprint(f"Action: {action}")  # 7-DOF joint positions\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"6-training-data-format",children:"6. Training Data Format"}),"\n",(0,t.jsx)(e.p,{children:"VLAs require large-scale datasets:"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Open X-Embodiment Dataset"})," (1M+ trajectories):"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"22 robot platforms"}),"\n",(0,t.jsx)(e.li,{children:"527 skills"}),"\n",(0,t.jsx)(e.li,{children:"160,000 tasks"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Data Format"})," (HDF5):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'{\n    "observations": {\n        "image": [224, 224, 3],\n        "state": [7],  # Joint positions\n    },\n    "actions": [7],  # Next joint positions\n    "language": "Pick up the cup"\n}\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"7-hands-on-lab-vla-inference-2-hours",children:"7. Hands-On Lab: VLA Inference (2 hours)"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Goal"}),": Run OpenVLA on sample images and instructions."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Install OpenVLA framework"}),"\n",(0,t.jsx)(e.li,{children:"Download pre-trained 7B model"}),"\n",(0,t.jsx)(e.li,{children:"Run inference on test images"}),"\n",(0,t.jsx)(e.li,{children:"Visualize predicted actions in RViz2"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Validation"}),": Model outputs 7-DOF actions for manipulation tasks"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"8-end-of-chapter-project",children:"8. End-of-Chapter Project"}),"\n",(0,t.jsx)(e.p,{children:"Run VLA on humanoid grasping dataset from Chapter 7."}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Load OpenVLA model"}),"\n",(0,t.jsx)(e.li,{children:"Process 100 test images"}),"\n",(0,t.jsx)(e.li,{children:"Compare predicted vs ground-truth actions"}),"\n",(0,t.jsx)(e.li,{children:"Calculate success rate (\xb110\xb0 joint tolerance)"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"VLAs enable language-conditioned robot control through vision-language-action alignment. Unlike traditional RL, VLAs generalize across tasks by training on massive multi-robot datasets."}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Next"}),": Chapter 14 covers fine-tuning OpenVLA on custom robot data."]})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var r=i(6540);const t={},s=r.createContext(t);function a(n){const e=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);