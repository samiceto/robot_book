"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[724],{2764:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"part4-vla/multimodal-reasoning","title":"Chapter 15: Multimodal Reasoning","description":"Learning Objectives","source":"@site/docs/part4-vla/15-multimodal-reasoning.md","sourceDirName":"part4-vla","slug":"/part4-vla/multimodal-reasoning","permalink":"/robot_book/docs/part4-vla/multimodal-reasoning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":15,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 14: OpenVLA Fine-Tuning","permalink":"/robot_book/docs/part4-vla/openvla-finetuning"},"next":{"title":"Chapter 16: End-to-End Visuomotor Control","permalink":"/robot_book/docs/part4-vla/visuomotor-control"}}');var s=t(4848),r=t(8453);const a={},l="Chapter 15: Multimodal Reasoning",o={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. VLM vs VLA",id:"1-vlm-vs-vla",level:2},{value:"2. Multimodal Pipeline",id:"2-multimodal-pipeline",level:2},{value:"3. GPT-4V Integration",id:"3-gpt-4v-integration",level:2},{value:"4. Chain-of-Thought Reasoning",id:"4-chain-of-thought-reasoning",level:2},{value:"5. VLM \u2192 VLA Pipeline",id:"5-vlm--vla-pipeline",level:2},{value:"6. Failure Recovery",id:"6-failure-recovery",level:2},{value:"7. Hierarchical Task Decomposition",id:"7-hierarchical-task-decomposition",level:2},{value:"8. Hands-On Lab: VLM-VLA Integration (3 hours)",id:"8-hands-on-lab-vlm-vla-integration-3-hours",level:2},{value:"9. End-of-Chapter Project",id:"9-end-of-chapter-project",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-15-multimodal-reasoning",children:"Chapter 15: Multimodal Reasoning"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate"})," VLMs (Vision-Language Models) for scene understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," chain-of-thought reasoning for complex tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Combine"})," VLM planning with VLA execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Handle"})," failure recovery with language feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build"})," hierarchical task decomposition systems"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-vlm-vs-vla",children:"1. VLM vs VLA"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model Type"}),(0,s.jsx)(n.th,{children:"Purpose"}),(0,s.jsx)(n.th,{children:"Example"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"VLM"})," (GPT-4V, LLaVA)"]}),(0,s.jsx)(n.td,{children:"Scene understanding, planning"}),(0,s.jsx)(n.td,{children:'"The cup is on the left side of the table"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"VLA"})," (OpenVLA)"]}),(0,s.jsx)(n.td,{children:"Action execution"}),(0,s.jsx)(n.td,{children:"Output: [0.2, 0.5, ...] joint positions"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Insight"}),': Use VLM for "what" and "why", VLA for "how"']}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-multimodal-pipeline",children:"2. Multimodal Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Camera Image + "Clean the table"\n        \u2193\n    VLM (GPT-4V)\n        \u2193\nTask Decomposition:\n1. "Locate dishes on table"\n2. "Pick up each dish"\n3. "Place in dishwasher"\n        \u2193\n    VLA (OpenVLA)\n        \u2193\nExecute each subtask\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-gpt-4v-integration",children:"3. GPT-4V Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nfrom PIL import Image\nimport base64\n\n# Encode image\nwith open("scene.jpg", "rb") as f:\n    image_b64 = base64.b64encode(f.read()).decode()\n\n# Query GPT-4V\nresponse = openai.ChatCompletion.create(\n    model="gpt-4-vision-preview",\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "Describe objects on the table and suggest grasping order."},\n                {"type": "image_url", "image_url": f"data:image/jpeg;base64,{image_b64}"}\n            ]\n        }\n    ]\n)\n\nplan = response.choices[0].message.content\nprint(plan)\n# "I see 3 objects: red cup (left), blue plate (center), green bottle (right).\n#  Suggested order: cup \u2192 plate \u2192 bottle (left to right)"\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-chain-of-thought-reasoning",children:"4. Chain-of-Thought Reasoning"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering"})," for step-by-step reasoning:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'prompt = """\nYou are a robot planning assistant. Given the image:\n\n1. List all objects visible\n2. Identify graspable objects\n3. Determine optimal grasp order\n4. Provide action sequence\n\nThink step-by-step before answering.\n"""\n\nresponse = query_gpt4v(image, prompt)\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example Output"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Step 1: Objects visible\n- Red ceramic mug (left side, handle facing right)\n- White plate with residue (center)\n- Green water bottle (right side, upright)\n\nStep 2: Graspable analysis\n- Mug: Yes (stable handle grasp)\n- Plate: Yes (edge grasp, but fragile)\n- Bottle: Yes (cylinder grasp)\n\nStep 3: Optimal order\n1. Mug (easiest, stable)\n2. Bottle (medium difficulty)\n3. Plate (hardest, requires delicate grasp)\n\nStep 4: Action sequence\nFOR mug: "Grasp red mug by handle"\nFOR bottle: "Grasp green bottle at center"\nFOR plate: "Grasp white plate at edge with two hands"\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"5-vlm--vla-pipeline",children:"5. VLM \u2192 VLA Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultimodalController:\n    def __init__(self):\n        self.vlm = GPT4V()        # Planning\n        self.vla = OpenVLA()      # Execution\n\n    def execute_task(self, image, task):\n        # 1. VLM: Generate plan\n        plan = self.vlm.plan(image, task)\n        steps = self.parse_plan(plan)\n\n        # 2. VLA: Execute each step\n        for step in steps:\n            action = self.vla.predict_action(\n                image=image,\n                instruction=step\n            )\n            success = self.robot.execute(action)\n\n            # 3. Failure recovery\n            if not success:\n                new_plan = self.vlm.replan(image, f"Failed: {step}. Suggest alternative.")\n                # ... retry\n\n        return success\n\n# Usage\ncontroller = MultimodalController()\nsuccess = controller.execute_task(\n    image=camera_image,\n    task="Clear the table"\n)\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"6-failure-recovery",children:"6. Failure Recovery"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Feedback Loop"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def execute_with_recovery(image, instruction, max_retries=3):\n    for attempt in range(max_retries):\n        # Execute\n        action = vla.predict_action(image, instruction)\n        success = robot.execute(action)\n\n        if success:\n            return True\n\n        # Get new image after failure\n        new_image = robot.get_camera_image()\n\n        # Ask VLM for recovery strategy\n        recovery_plan = vlm.query(\n            new_image,\n            f\"Execution failed for '{instruction}'. What went wrong? Suggest fix.\"\n        )\n\n        # Update instruction based on feedback\n        instruction = recovery_plan\n\n    return False\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"7-hierarchical-task-decomposition",children:"7. Hierarchical Task Decomposition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# High-level task\ntask = "Prepare breakfast"\n\n# VLM decomposes into subtasks\nsubtasks = vlm.decompose(task)\n# ["Locate cereal box", "Grasp cereal box", "Pour cereal into bowl",\n#  "Locate milk", "Grasp milk", "Pour milk into bowl"]\n\n# Execute each subtask with VLA\nfor subtask in subtasks:\n    image = robot.get_camera_image()\n    action = vla.predict_action(image, subtask)\n    robot.execute(action)\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"8-hands-on-lab-vlm-vla-integration-3-hours",children:"8. Hands-On Lab: VLM-VLA Integration (3 hours)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Build multimodal system for tabletop clearing."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Setup GPT-4V API access"}),"\n",(0,s.jsx)(n.li,{children:"Implement VLM \u2192 VLA pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Test on 10 cluttered table scenes"}),"\n",(0,s.jsx)(n.li,{children:"Measure success rate with/without VLM planning"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Validation"}),": VLM planning improves success by 20%+"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"9-end-of-chapter-project",children:"9. End-of-Chapter Project"}),"\n",(0,s.jsx)(n.p,{children:"Build autonomous kitchen assistant with multimodal reasoning."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"VLM: Identify dishes, utensils, food items"}),"\n",(0,s.jsx)(n.li,{children:"VLA: Execute grasping and placement"}),"\n",(0,s.jsx)(n.li,{children:"Hierarchical decomposition for multi-step tasks"}),"\n",(0,s.jsx)(n.li,{children:"Failure recovery with replanning"}),"\n",(0,s.jsx)(n.li,{children:"Success rate >65% on 20 test scenarios"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deliverables"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multimodal pipeline code"}),"\n",(0,s.jsx)(n.li,{children:"Evaluation report"}),"\n",(0,s.jsx)(n.li,{children:"Demo video (breakfast preparation or table clearing)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Multimodal reasoning combines VLM planning (semantic understanding) with VLA execution (precise control). This enables complex, long-horizon tasks beyond single-step manipulation."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": Chapter 16 covers end-to-end visuomotor control and real-world deployment."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);