"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[87],{5359:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"part4-vla/visuomotor-control","title":"Chapter 16: End-to-End Visuomotor Control","description":"Learning Objectives","source":"@site/docs/part4-vla/16-visuomotor-control.md","sourceDirName":"part4-vla","slug":"/part4-vla/visuomotor-control","permalink":"/robot_book/docs/part4-vla/visuomotor-control","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":16,"frontMatter":{},"sidebar":"bookSidebar","previous":{"title":"Chapter 15: Multimodal Reasoning","permalink":"/robot_book/docs/part4-vla/multimodal-reasoning"},"next":{"title":"Chapter 17: Bipedal Locomotion","permalink":"/robot_book/docs/part5-advanced/bipedal-locomotion"}}');var o=i(4848),r=i(8453);const s={},l="Chapter 16: End-to-End Visuomotor Control",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. What is Visuomotor Control?",id:"1-what-is-visuomotor-control",level:2},{value:"2. Real-Time Requirements",id:"2-real-time-requirements",level:2},{value:"3. Inference Optimization",id:"3-inference-optimization",level:2},{value:"Quantization (FP32 \u2192 INT8)",id:"quantization-fp32--int8",level:3},{value:"Model Distillation (7B \u2192 1B)",id:"model-distillation-7b--1b",level:3},{value:"4. Sim-to-Real Transfer",id:"4-sim-to-real-transfer",level:2},{value:"1. Domain Randomization (Chapter 7)",id:"1-domain-randomization-chapter-7",level:3},{value:"2. Real-World Fine-Tuning",id:"2-real-world-fine-tuning",level:3},{value:"5. ROS 2 Integration",id:"5-ros-2-integration",level:2},{value:"6. Safety and Monitoring",id:"6-safety-and-monitoring",level:2},{value:"Collision Detection",id:"collision-detection",level:3},{value:"Emergency Stop",id:"emergency-stop",level:3},{value:"7. Hands-On Lab: Real Robot Deployment (4 hours)",id:"7-hands-on-lab-real-robot-deployment-4-hours",level:2},{value:"8. Evaluation Metrics",id:"8-evaluation-metrics",level:2},{value:"Success Rate",id:"success-rate",level:3},{value:"Execution Time",id:"execution-time",level:3},{value:"9. End-of-Chapter Project",id:"9-end-of-chapter-project",level:2},{value:"Summary",id:"summary",level:2},{value:"Part 4 Complete!",id:"part-4-complete",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-16-end-to-end-visuomotor-control",children:"Chapter 16: End-to-End Visuomotor Control"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Deploy"})," VLA models on real humanoid robots"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Implement"})," closed-loop visuomotor control at 10+ Hz"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Handle"})," sim-to-real transfer challenges"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Optimize"})," inference latency for real-time control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Evaluate"})," performance on physical manipulation tasks"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"1-what-is-visuomotor-control",children:"1. What is Visuomotor Control?"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": Direct mapping from visual observations to motor commands without explicit perception or planning stages."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Raw Pixels \u2192 Neural Network \u2192 Robot Actions\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Benefits"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"End-to-end learning"}),"\n",(0,o.jsx)(e.li,{children:"No manual feature engineering"}),"\n",(0,o.jsx)(e.li,{children:"Handles uncertainty implicitly"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"2-real-time-requirements",children:"2. Real-Time Requirements"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Control Loop"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"1. Capture image (30 ms)\n2. VLA inference (50-100 ms)\n3. Execute action (30 ms)\nTotal: ~100-160 ms \u2192 6-10 Hz\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Target"}),": \u226510 Hz for stable manipulation"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"3-inference-optimization",children:"3. Inference Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"quantization-fp32--int8",children:"Quantization (FP32 \u2192 INT8)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from openvla import OpenVLA\nimport torch\n\n# Load model\nmodel = OpenVLA.from_pretrained("openvla/openvla-7b")\n\n# Quantize to INT8\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model,\n    {torch.nn.Linear},\n    dtype=torch.qint8\n)\n\n# Benchmark\nimport time\nstart = time.time()\naction = model_int8.predict_action(image, instruction)\nlatency = time.time() - start\nprint(f"Latency: {latency*1000:.1f} ms")  # 50-80 ms on RTX 4090\n'})}),"\n",(0,o.jsx)(e.h3,{id:"model-distillation-7b--1b",children:"Model Distillation (7B \u2192 1B)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Train smaller model to mimic large model\nteacher = OpenVLA.from_pretrained("openvla/openvla-7b")\nstudent = OpenVLA.from_pretrained("openvla/openvla-1b")\n\n# Distillation loss\ndef distillation_loss(student_logits, teacher_logits, temperature=2.0):\n    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)\n    soft_student = F.log_softmax(student_logits / temperature, dim=-1)\n    return F.kl_div(soft_student, soft_targets, reduction=\'batchmean\')\n\n# Train student to match teacher\n# Result: 3x faster inference, ~5% accuracy drop\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"4-sim-to-real-transfer",children:"4. Sim-to-Real Transfer"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Domain Gap"}),":"]}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Aspect"}),(0,o.jsx)(e.th,{children:"Simulation"}),(0,o.jsx)(e.th,{children:"Reality"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Lighting"}),(0,o.jsx)(e.td,{children:"Perfect"}),(0,o.jsx)(e.td,{children:"Variable"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Physics"}),(0,o.jsx)(e.td,{children:"Exact"}),(0,o.jsx)(e.td,{children:"Noisy"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Sensing"}),(0,o.jsx)(e.td,{children:"Ideal"}),(0,o.jsx)(e.td,{children:"Delays, noise"})]})]})]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Mitigation Strategies"}),":"]}),"\n",(0,o.jsx)(e.h3,{id:"1-domain-randomization-chapter-7",children:"1. Domain Randomization (Chapter 7)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Randomize in sim during training\nrandomize_lighting(intensity=(500, 2000))\nrandomize_textures()\nrandomize_physics(friction=(0.3, 0.9))\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-real-world-fine-tuning",children:"2. Real-World Fine-Tuning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Collect 50-100 real demos\nreal_data = collect_teleoperation_data(num_episodes=100)\n\n# Fine-tune sim-trained model\nmodel.fine_tune(\n    real_data,\n    learning_rate=1e-5,\n    epochs=5\n)\n# Improves success: 45% \u2192 75%\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"5-ros-2-integration",children:"5. ROS 2 Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom cv_bridge import CvBridge\nfrom openvla import OpenVLA\n\nclass VisuomotorController(Node):\n    def __init__(self):\n        super().__init__('visuomotor_controller')\n\n        # Load VLA model\n        self.model = OpenVLA.from_pretrained(\"humanoid_vla_lora\")\n        self.model = self.model.to(\"cuda\")\n\n        # Current instruction\n        self.instruction = \"Pick up the cup\"\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10)\n\n        # Publishers\n        self.joint_pub = self.create_publisher(\n            JointState, '/joint_commands', 10)\n\n        self.bridge = CvBridge()\n        self.get_logger().info('Visuomotor controller ready')\n\n    def image_callback(self, msg):\n        # Convert to numpy\n        image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\n\n        # VLA inference\n        action = self.model.predict_action(\n            image=image,\n            instruction=self.instruction\n        )\n\n        # Publish joint commands\n        joint_msg = JointState()\n        joint_msg.position = action.tolist()\n        self.joint_pub.publish(joint_msg)\n\ndef main():\n    rclpy.init()\n    node = VisuomotorController()\n    rclpy.spin(node)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"6-safety-and-monitoring",children:"6. Safety and Monitoring"}),"\n",(0,o.jsx)(e.h3,{id:"collision-detection",children:"Collision Detection"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"def safe_execute(action, current_joints):\n    # Check joint limits\n    if not within_limits(action):\n        return None\n\n    # Predict next state\n    next_state = forward_kinematics(action)\n\n    # Check self-collision\n    if self_collision(next_state):\n        return None\n\n    # Check workspace bounds\n    if out_of_bounds(next_state):\n        return None\n\n    return action\n"})}),"\n",(0,o.jsx)(e.h3,{id:"emergency-stop",children:"Emergency Stop"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class SafetyMonitor:\n    def __init__(self):\n        self.force_threshold = 50.0  # Newtons\n        self.velocity_threshold = 1.0  # m/s\n\n    def check_safety(self, force, velocity):\n        if force > self.force_threshold:\n            robot.emergency_stop()\n            return False\n\n        if velocity > self.velocity_threshold:\n            robot.slow_down(factor=0.5)\n\n        return True\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"7-hands-on-lab-real-robot-deployment-4-hours",children:"7. Hands-On Lab: Real Robot Deployment (4 hours)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Goal"}),": Deploy VLA to physical humanoid for grasping."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Setup RealSense + humanoid arm"}),"\n",(0,o.jsx)(e.li,{children:"Load fine-tuned VLA model"}),"\n",(0,o.jsx)(e.li,{children:"Implement visuomotor control node"}),"\n",(0,o.jsx)(e.li,{children:"Test on 20 grasping scenarios"}),"\n",(0,o.jsx)(e.li,{children:"Measure success rate and latency"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Validation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Inference: <100 ms"}),"\n",(0,o.jsx)(e.li,{children:"Control frequency: >10 Hz"}),"\n",(0,o.jsx)(e.li,{children:"Success rate: >60%"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"8-evaluation-metrics",children:"8. Evaluation Metrics"}),"\n",(0,o.jsx)(e.h3,{id:"success-rate",children:"Success Rate"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'successes = 0\nfor trial in range(100):\n    success = execute_task(object, instruction)\n    if success:\n        successes += 1\n\nsuccess_rate = successes / 100\nprint(f"Success Rate: {success_rate:.1%}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"execution-time",children:"Execution Time"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'times = []\nfor trial in range(100):\n    start = time.time()\n    execute_task(object, instruction)\n    times.append(time.time() - start)\n\nprint(f"Mean: {np.mean(times):.2f}s")\nprint(f"Std: {np.std(times):.2f}s")\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"9-end-of-chapter-project",children:"9. End-of-Chapter Project"}),"\n",(0,o.jsx)(e.p,{children:"Deploy complete visuomotor control system on humanoid."}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"VLA model (fine-tuned from Chapter 14)"}),"\n",(0,o.jsx)(e.li,{children:"ROS 2 integration with RealSense"}),"\n",(0,o.jsx)(e.li,{children:"Real-time control at \u226510 Hz"}),"\n",(0,o.jsx)(e.li,{children:"Safety monitoring (collision, limits)"}),"\n",(0,o.jsx)(e.li,{children:"Test on 50 real-world manipulation tasks"}),"\n",(0,o.jsx)(e.li,{children:"Success rate >55%"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Deliverables"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Deployment code (ROS 2 package)"}),"\n",(0,o.jsx)(e.li,{children:"Performance report (success rate, latency)"}),"\n",(0,o.jsx)(e.li,{children:"Safety analysis"}),"\n",(0,o.jsx)(e.li,{children:"5-minute demo video"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"End-to-end visuomotor control enables direct pixel-to-action policies for robot manipulation. With proper optimization (quantization, distillation) and sim-to-real transfer (domain randomization, real fine-tuning), VLAs achieve 60-80% success on real humanoid tasks."}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"part-4-complete",children:"Part 4 Complete!"}),"\n",(0,o.jsx)(e.p,{children:"You've learned:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"VLA architecture and components"}),"\n",(0,o.jsx)(e.li,{children:"Fine-tuning with LoRA"}),"\n",(0,o.jsx)(e.li,{children:"Multimodal reasoning (VLM + VLA)"}),"\n",(0,o.jsx)(e.li,{children:"Real-time visuomotor control"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Overall Progress"}),": 16 of 21 chapters (76%)"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Next"}),": Part 5 (Advanced Topics) covers locomotion, whole-body control, human-robot interaction, and deployment at scale."]})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>l});var t=i(6540);const o={},r=t.createContext(o);function s(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);